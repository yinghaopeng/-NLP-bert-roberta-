{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  1、项目背景介绍\n",
    "在我们日常生活中，我们使用邮件进行信息传递，就好比使用QQ、微信等聊天软件一样进行双方隔空对话，但是邮箱其实对于我们来说其实更大的作用，在于我们和对方进行一些重要信息沟通，但是往往在我们的邮箱时不时存在一些垃圾邮件，而这些垃圾邮件的来源可以说是，五花八门，因为以笔者自身的例子作为一个案例分析，笔者在中考后购买了苹果的手机，当时就是使用自己的QQ邮箱进行注册，自从那天开始，后面苹果每逢周一或者是遇到新的产品上市，或者是新的游戏上市app store，则会以邮件的方式推送给我，对于用户的我来说，我觉得无疑是会影响我的邮箱使用，再者当收到邮件时候，邮箱给我发送邮件提醒，甚至会误导我以为是一些重要的邮件，又或者笔者之前才加coursera的课程，当时也是用QQ邮箱进行注册登录，后面也是不定时的给我发送一些推销它们产品的广告，我觉得这种垃圾邮件一来会对邮箱的容量进行占据，另外一方面会让使用者降低对邮件的使用频率，因此笔者以垃圾邮件的数据集作为本次项目的训练，并希望日后能够用上部署在一些邮箱软件上，可以让软件自动帮我们识别发过来的邮件信息，并进行一定的过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 这个是笔者第一次独立完成的项目，希望可以fork一下哦！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2、数据介绍\n",
    "[TREC2005-2007垃圾邮件数据集](https://aistudio.baidu.com/aistudio/datasetdetail/89631/1)，原数据集描述：是一个公开的垃圾邮件语料库，由国际文本检索会议提供，分为英文数据集（trec06p）和中文数据集（trec06c），其中所含的邮件均来源于真实邮件保留了邮件的原有格式和内容。除TREC 2006外，还有TREC 2005和TREC 2007的英文垃圾邮件数据集，因为本文主要应对的还是对于中文邮件，因此主要是使用垃圾邮件的中文数据集trec06c作为研究对象，也可以从官网上获取其[数据](https://plg.uwaterloo.ca/~gvcormac/treccorpus06/about.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3、模型介绍\n",
    "本文一共使用两个模型进行对比训练，一个是bert模型一个是roberta模型进行对比训练，通过visualdl可视化工具对两种的模型进行观察，并给出哪一种模型较优\n",
    "## 3.1bert模型\n",
    "### 3.1.1什么是bert？\n",
    "BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。\n",
    "\n",
    "它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的masked language model（MLM），因此能生成深度的双向语言表征。\n",
    "\n",
    "该模型有以下主要优点：\n",
    "\n",
    "1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。\n",
    "\n",
    "2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。\n",
    "### 3.1.2bert模型结构\n",
    "以往的预训练模型的结构会受到单向语言模型（从左到右或者从右到左）的限制，因而也限制了模型的表征能力，使其只能获取单方向的上下文信息。\n",
    "\n",
    "而BERT利用MLM进行预训练并且采用深层的双向Transformer组件来构建整个模型，因此最终生成能融合左右上下文信息的深层双向语言表征。\n",
    "> 注：单向的Transformer一般被称为Transformer decoder，其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个token会attend到所有的token。\n",
    "\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/31eb385fe74d4466a2ca9ae0a279ac5a3c4ac29b823142c99b220d0c3a0579dd\" width=\"  \"></div>\n",
    "<center>Transformers模型结构</center>\n",
    "\n",
    "Transformer进行堆叠，形成一个更深的神经网络，如下图所示\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e27076146565415395af046993f39c87177f0552fa154b248c1ad93569c5cfc5\" width=\"  \"></div>\n",
    "<center>对Transformers进行堆叠</center>\n",
    "\n",
    "最终，经过多层Transformer的堆叠后bert的主体如下所示\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d33407faede14fc49908c515c72b763e588e1ca569b343e8a85949ea71379c00\" width=\"  \"></div>\n",
    "<center>bert主体结构</center>\n",
    "\n",
    "### 3.1.3bert模型应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入库函数\r\n",
    "import re\r\n",
    "import jieba\r\n",
    "import os \r\n",
    "import random\r\n",
    "import paddle\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.data import Stack, Pad, Tuple\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.nn as nn\r\n",
    "from visualdl import LogWriter\r\n",
    "import numpy as np\r\n",
    "from functools import partial #partial()函数可以用来固定某些参数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle's version: 2.2.2\n",
      "paddlenlp's version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# 查看paddle和paddlenlp的版本\r\n",
    "print(\"paddle's version:\", paddle.__version__)\r\n",
    "print(\"paddlenlp's version:\", ppnlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据集\r\n",
    "!tar xf data/data89631/trec06c.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 去掉非中文字符\r\n",
    "def clean_str(string):\r\n",
    "    string = re.sub(r\"[^\\u4e00-\\u9fff]\", \" \", string)\r\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\r\n",
    "    return string.strip()\r\n",
    "\r\n",
    "# 读取邮件文件内容信息\r\n",
    "def get_data_in_a_file(original_path, save_path = 'all_email.txt'):\r\n",
    "    email = ''\r\n",
    "    f = open(original_path, 'r', encoding = 'gb2312', errors = 'ignore')#使用ignore参数可以防止读入数据发生部分字符无法读入\r\n",
    "    for line in f:\r\n",
    "        line = line.strip().strip('\\n')# 去掉换行符\r\n",
    "        line = clean_str(line)# 去掉非中文字符\r\n",
    "        email += line\r\n",
    "    f.close()\r\n",
    "    return email[-200:]# 只保留末尾200个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取标签文件信息\r\n",
    "file_index = open('trec06c/full/index', 'r')\r\n",
    "for line in file_index:\r\n",
    "    str_list = line.split(\" \")\r\n",
    "    if str_list[0] == 'spam':# 垃圾邮件标签为0\r\n",
    "        label = '0'\r\n",
    "    elif str_list[0] == 'ham':# 正常邮件标签为1\r\n",
    "        label = '1'\r\n",
    "    text = get_data_in_a_file('trec06c/full/' + str(str_list[1].split(\"\\n\")[0]))\r\n",
    "    with open(\"all_email.txt\",\"a+\") as file_index:\r\n",
    "                    file_index.write(text + '\\t' + label + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data have completed\n"
     ]
    }
   ],
   "source": [
    "data_list_path=\"./\"\r\n",
    "\r\n",
    "with open(os.path.join(data_list_path, 'eval_list.txt'), 'w', encoding='utf-8') as f_eval:\r\n",
    "    f_eval.seek(0)\r\n",
    "    f_eval.truncate()\r\n",
    "    \r\n",
    "with open(os.path.join(data_list_path, 'train_list.txt'), 'w', encoding='utf-8') as f_train:\r\n",
    "    f_train.seek(0)\r\n",
    "    f_train.truncate() \r\n",
    "\r\n",
    "with open(os.path.join(data_list_path, 'test_list.txt'), 'w', encoding='utf-8') as f_test:\r\n",
    "    f_test.seek(0)\r\n",
    "    f_test.truncate()\r\n",
    "\r\n",
    "with open(os.path.join(data_list_path, 'all_email.txt'), 'r', encoding='utf-8') as f_data:\r\n",
    "    lines = f_data.readlines()\r\n",
    "\r\n",
    "i = 0\r\n",
    "with open(os.path.join(data_list_path, 'eval_list.txt'), 'a', encoding='utf-8') as f_eval,open(os.path.join(data_list_path, 'test_list.txt'), 'a', encoding='utf-8') as f_test,open(os.path.join(data_list_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\r\n",
    "    for line in lines:\r\n",
    "        label = line.split('\\t')[-1].replace('\\n', '')# 提取label信息\r\n",
    "        words = line.split('\\t')[0]# 提取输入文本信息\r\n",
    "        words = words.replace(' ', ',') # 邮件文本空格用逗号替换\r\n",
    "        labs = \"\"\r\n",
    "        # 数据清洗，如果输入文本内容为空，在BERT模型finetune时报错\r\n",
    "        if len(words) > 0:\r\n",
    "            if i % 10 == 1:# 划分验证集\r\n",
    "                labs = words + '\\t' + label + '\\n'\r\n",
    "                f_eval.write(labs)\r\n",
    "            elif i % 10 == 2:# 划分测试集\r\n",
    "                labs = words + '\\t' + label + '\\n'\r\n",
    "                f_test.write(labs)\r\n",
    "            else: # 划分训练集\r\n",
    "                labs = words + '\\t' + label + '\\n'\r\n",
    "                f_train.write(labs)\r\n",
    "            i += 1\r\n",
    "        else:\r\n",
    "            pass\r\n",
    "    \r\n",
    "print(\"data have completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从本地文件创建数据集\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "# 重写read函数\r\n",
    "def read(data_path):\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "        next(f)# 跳过列名\r\n",
    "        for line in f:\r\n",
    "            words, labels = line.strip('\\n').split('\\t')\r\n",
    "            words = words.split('\\002')\r\n",
    "            labels = labels.split('\\002')\r\n",
    "            yield {'text': words[0], 'label': labels[0]}\r\n",
    "\r\n",
    "# data_path为read()方法的参数\r\n",
    "train_ds = load_dataset(read,data_path='train_list.txt',splits='train',lazy=False)# 训练集\r\n",
    "dev_ds = load_dataset(read,data_path='eval_list.txt',splits='dev',lazy=False)# 验证集\r\n",
    "test_ds = load_dataset(read,data_path='test_list.txt',splits='test',lazy=False)# 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据：[{'text': '贵公司负责人,经理,财务,您好深圳市华龙公司受多家公司委托向外低点代开部分增值税电脑发票,左右,和普通商品销售税发票,国税,地税运输,广告,服务等票,左右,还可以根据所做数量额度的大小来商讨优惠的点数本公司郑重承诺所用绝对是真票,可验证后付款此信息长期有效,如须进一步洽商请电联系人,刘剑辉顺祝商祺低点代开发票', 'label': '0'}, {'text': '用付出劳动,那就交注册费吧,呵呵,让网站去赚你注册费的,吧,你注册费的,付给你的上线,那样你真的赚到什么了吗,真搞不懂当您发展下线时,只需将本页的注册连接中的,换成您在,的用户名即可独乐乐,不如众乐乐,大家一起赚美国人的钱吧把这个连接,全部蓝色部份,复制到浏览器地址栏中,回车即可进入注册界面我的邮件地址广告,网络电话包年卡,元,长途市话全包最快的论坛邮址搜索专家,最好的邮件群发专家论坛短信群发专家', 'label': '0'}, {'text': '贵公司经理,财务您好深圳市春洋贸易有限公司,东莞分公司我司本着互惠互利的优势和良好的社会关系,得到了社会各界人士的认同因本公司进项较多,为要冲减进项,现有,增值税,电脑,发票和海关代征增值税,专用缴款书对外提供,其它,国税,地税,等普通发票可优惠对外代开或合作,以上承诺所有票据均可上网查询或到税务局抵扣验证本信息长期有效,信誉第一,欢迎来人来电洽谈联,系,人,李,生咨询电话传,真祝商祺', 'label': '0'}]\n",
      "\n",
      "验证集数据:[{'text': '贵公司负责人,经理,财务,您好深圳市华龙公司受多家公司委托向外低点代开部分增值税电脑发票,左右,和普通商品销售税发票,国税,地税运输,广告,服务等票,左右,还可以根据所做数量额度的大小来商讨优惠的点数本公司郑重承诺所用绝对是真票,可验证后付款此信息长期有效,如须进一步洽商请电联系人,刘剑辉顺祝商祺低点代开发票', 'label': '0'}, {'text': '可以代理代办其它发票如,广告,运输,建筑其它服务行业都可以代理代办,我公司因全年为外商代理进出口业务,所开的税额用海关缴款书在当地税务部门已抵税,等于我司纳税后才开出,正常我司的税收点数比较低,请各公司放心我公司都有正当手续,如有希要以上业务的公司,厂家,请向我司主管人员联系本公司向所有公司,厂家,承诺先验票后付款,真诚期待与贵公司,厂家,合作欢迎来电咨询深圳协恒实业有限公司联系人,张永辉联系电话', 'label': '0'}, {'text': '会关系,因本公司进项较多,现完成不了每月销售额度,为要冲减进项,现有,增值税,电脑,发票对外提供,税率方面较低,左右的税点,其它,国税,地税,电脑运输等普通发票,左右的税点优惠代开或合作,还可以根据数目的大小来衡量优惠的多少本公司郑重承诺所用票据均可上网查询或到税务局抵扣验证彼此合作一次,必成永久朋友此信息长期有效,如须进一步洽商,欢迎来电垂询请电,邮箱联系人,金振南顺祝商祺深圳市华雄实业有限公司', 'label': '0'}]\n",
      "\n",
      "测试集数据:[{'text': '您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺', 'label': '0'}, {'text': ',负责人您好我是深圳联美实业有限公司,广州,东莞,等省市有分公司我司有良好的社会关系和实力,因每月进项多出项少现有一部分发票可优惠对外代开税率较低,增值税发票为,其它国税,地税运输,广告等普通发票为,的税点,还可以根据数目大小来衡量优惠的多少,希望贵公司,商家等来电商谈欢迎合作本公司郑重承诺所用票据可到税务局验证或抵扣欢迎来电进一步商谈电话,小时服务信箱联系人,郭江河顺祝商祺深圳市联美实业有限公司', 'label': '0'}, {'text': '您好,本公司经营国内商业,物资供销业及财税信息咨询服务为主现因进项余额,可为贵司以较低,税率代开下列发票运输发票,电脑版,可抵扣普通商品销售发票,广告业专用发票,建筑安装发票海关,增值,缴款书,其他服务行业专用发票等贵司可用上述发票作销售产品或冲减帐目之用,如有此项业务需求可来电来函咨询祝商祺联系人,贺铭坤电,话传,真上海市恒源实业有限公司', 'label': '0'}]\n",
      "\n",
      "训练集样本个数:49509\n",
      "验证集样本个数:6188\n",
      "测试集样本个数:6188\n"
     ]
    }
   ],
   "source": [
    "#看看数据长什么样子，分别打印训练集、验证集、测试集的前3条数据。\r\n",
    "print(\"训练集数据：{}\\n\".format(train_ds[0:3]))\r\n",
    "print(\"验证集数据:{}\\n\".format(dev_ds[0:3]))\r\n",
    "print(\"测试集数据:{}\\n\".format(test_ds[0:3]))\r\n",
    "\r\n",
    "print(\"训练集样本个数:{}\".format(len(train_ds)))\r\n",
    "print(\"验证集样本个数:{}\".format(len(dev_ds)))\r\n",
    "print(\"测试集样本个数:{}\".format(len(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-27 13:51:19,569] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# bert模型的token\r\n",
    "tokenizer_bert = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据预处理\r\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\r\n",
    "    \"\"\"\r\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \r\n",
    "    to be used in a sequence-pair classification task.\r\n",
    "        \r\n",
    "    A BERT sequence has the following format:\r\n",
    "\r\n",
    "    - single sequence: ``[CLS] X [SEP]``\r\n",
    "    - pair of sequences: ``[CLS] A [SEP] B [SEP]``\r\n",
    "\r\n",
    "    A BERT sequence pair mask has the following format:\r\n",
    "    ::\r\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\r\n",
    "        | first sequence    | second sequence |\r\n",
    "\r\n",
    "    If only one sequence, only returns the first portion of the mask (0's).\r\n",
    "\r\n",
    "\r\n",
    "    Args:\r\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\r\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \r\n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\r\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \r\n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\r\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\r\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\r\n",
    "        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.\r\n",
    "    \"\"\"\r\n",
    "    encoded_inputs = tokenizer(text=example[\"text\"], max_seq_len=max_seq_length)\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        return input_ids, token_type_ids\r\n",
    "# 数据迭代器\r\n",
    "def create_dataloader(dataset,\r\n",
    "                      mode='train',\r\n",
    "                      batch_size=1,\r\n",
    "                      batchify_fn=None,\r\n",
    "                      trans_fn=None):\r\n",
    "    if trans_fn:\r\n",
    "        dataset = dataset.map(trans_fn)\r\n",
    "\r\n",
    "    shuffle = True if mode == 'train' else False\r\n",
    "    if mode == 'train':\r\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\r\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    else:\r\n",
    "        batch_sampler = paddle.io.BatchSampler(\r\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\r\n",
    "\r\n",
    "    return paddle.io.DataLoader(\r\n",
    "        dataset=dataset,\r\n",
    "        batch_sampler=batch_sampler,\r\n",
    "        collate_fn=batchify_fn,\r\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用partial()来固定convert_example函数的tokenizer, max_seq_length, is_test等参数值\r\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer_bert, max_seq_length=128, is_test=False)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(Pad(axis=0,pad_val=tokenizer_bert.pad_token_id), Pad(axis=0, pad_val=tokenizer_bert.pad_token_id), Stack(dtype=\"int64\")):[data for data in fn(samples)]\r\n",
    "#训练集迭代器\r\n",
    "train_loader = create_dataloader(train_ds, mode='train', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\r\n",
    "#验证集迭代器\r\n",
    "dev_loader = create_dataloader(dev_ds, mode='dev', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\r\n",
    "#测试集迭代器\r\n",
    "test_loader = create_dataloader(test_ds, mode='test', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-27 13:51:30,021] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\n",
      "W0227 13:51:30.026201  3786 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0227 13:51:30.031549  3786 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "# 加载BERT预训练模型\r\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13850/2714257821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnum_training_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarmup_proption\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "#设置训练超参数\r\n",
    "\r\n",
    "#学习率\r\n",
    "learning_rate = 1e-5 \r\n",
    "#训练轮次\r\n",
    "epochs = 10\r\n",
    "#学习率预热比率\r\n",
    "warmup_proption = 0.1\r\n",
    "#权重衰减系数\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_loader) * epochs\r\n",
    "num_warmup_steps = int(warmup_proption * num_training_steps)\r\n",
    "\r\n",
    "def get_lr_factor(current_step):\r\n",
    "    if current_step < num_warmup_steps:\r\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\r\n",
    "    else:\r\n",
    "        return max(0.0,\r\n",
    "                    float(num_training_steps - current_step) /\r\n",
    "                    float(max(1, num_training_steps - num_warmup_steps)))\r\n",
    "#学习率调度器\r\n",
    "lr_scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate, lr_lambda=lambda current_step: get_lr_factor(current_step))\r\n",
    "\r\n",
    "#优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ])\r\n",
    "\r\n",
    "#损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "#评估函数\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#评估函数，设置返回值，便于VisualDL记录\r\n",
    "def evaluate(model, criterion, metric, data_loader):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    losses = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        losses.append(loss.numpy())\r\n",
    "        correct = metric.compute(logits, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        accu = metric.accumulate()\r\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\r\n",
    "    model.train()\r\n",
    "    metric.reset()\r\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 50, epoch: 1, batch: 50, loss: 0.52689, acc: 0.63969\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.34299, acc: 0.70406\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.15050, acc: 0.78177\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.05508, acc: 0.82445\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.05486, acc: 0.85337\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.07782, acc: 0.87255\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.09382, acc: 0.88746\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.08709, acc: 0.89891\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.01485, acc: 0.90785\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.09724, acc: 0.91509\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.01125, acc: 0.92148\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.10252, acc: 0.92667\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.02173, acc: 0.93130\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.08891, acc: 0.93542\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.08660, acc: 0.93871\n",
      "eval loss: 0.03867, accu: 0.98853\n",
      "global step 800, epoch: 2, batch: 26, loss: 0.06467, acc: 0.99279\n",
      "global step 850, epoch: 2, batch: 76, loss: 0.12111, acc: 0.99178\n",
      "global step 900, epoch: 2, batch: 126, loss: 0.03487, acc: 0.99107\n",
      "global step 950, epoch: 2, batch: 176, loss: 0.00612, acc: 0.99130\n",
      "global step 1000, epoch: 2, batch: 226, loss: 0.00365, acc: 0.99150\n",
      "global step 1050, epoch: 2, batch: 276, loss: 0.02627, acc: 0.99139\n",
      "global step 1100, epoch: 2, batch: 326, loss: 0.09561, acc: 0.99185\n",
      "global step 1150, epoch: 2, batch: 376, loss: 0.00201, acc: 0.99210\n",
      "global step 1200, epoch: 2, batch: 426, loss: 0.03447, acc: 0.99222\n",
      "global step 1250, epoch: 2, batch: 476, loss: 0.00298, acc: 0.99219\n",
      "global step 1300, epoch: 2, batch: 526, loss: 0.01909, acc: 0.99222\n",
      "global step 1350, epoch: 2, batch: 576, loss: 0.02940, acc: 0.99227\n",
      "global step 1400, epoch: 2, batch: 626, loss: 0.00357, acc: 0.99221\n",
      "global step 1450, epoch: 2, batch: 676, loss: 0.00310, acc: 0.99226\n",
      "global step 1500, epoch: 2, batch: 726, loss: 0.02597, acc: 0.99227\n",
      "eval loss: 0.02144, accu: 0.99418\n",
      "global step 1550, epoch: 3, batch: 2, loss: 0.00991, acc: 1.00000\n",
      "global step 1600, epoch: 3, batch: 52, loss: 0.00249, acc: 0.99669\n",
      "global step 1650, epoch: 3, batch: 102, loss: 0.01074, acc: 0.99632\n",
      "global step 1700, epoch: 3, batch: 152, loss: 0.00146, acc: 0.99640\n",
      "global step 1750, epoch: 3, batch: 202, loss: 0.00613, acc: 0.99660\n",
      "global step 1800, epoch: 3, batch: 252, loss: 0.00214, acc: 0.99696\n",
      "global step 1850, epoch: 3, batch: 302, loss: 0.09657, acc: 0.99690\n",
      "global step 1900, epoch: 3, batch: 352, loss: 0.00182, acc: 0.99676\n",
      "global step 1950, epoch: 3, batch: 402, loss: 0.00122, acc: 0.99674\n",
      "global step 2000, epoch: 3, batch: 452, loss: 0.00713, acc: 0.99685\n",
      "global step 2050, epoch: 3, batch: 502, loss: 0.00335, acc: 0.99704\n",
      "global step 2100, epoch: 3, batch: 552, loss: 0.01610, acc: 0.99703\n",
      "global step 2150, epoch: 3, batch: 602, loss: 0.00275, acc: 0.99704\n",
      "global step 2200, epoch: 3, batch: 652, loss: 0.00276, acc: 0.99712\n",
      "global step 2250, epoch: 3, batch: 702, loss: 0.00369, acc: 0.99708\n",
      "global step 2300, epoch: 3, batch: 752, loss: 0.00137, acc: 0.99711\n",
      "eval loss: 0.01409, accu: 0.99580\n",
      "global step 2350, epoch: 4, batch: 28, loss: 0.00194, acc: 0.99944\n",
      "global step 2400, epoch: 4, batch: 78, loss: 0.00120, acc: 0.99940\n",
      "global step 2450, epoch: 4, batch: 128, loss: 0.00068, acc: 0.99902\n",
      "global step 2500, epoch: 4, batch: 178, loss: 0.00165, acc: 0.99895\n",
      "global step 2550, epoch: 4, batch: 228, loss: 0.00072, acc: 0.99904\n",
      "global step 2600, epoch: 4, batch: 278, loss: 0.00215, acc: 0.99893\n",
      "global step 2650, epoch: 4, batch: 328, loss: 0.00041, acc: 0.99900\n",
      "global step 2700, epoch: 4, batch: 378, loss: 0.00119, acc: 0.99909\n",
      "global step 2750, epoch: 4, batch: 428, loss: 0.00132, acc: 0.99916\n",
      "global step 2800, epoch: 4, batch: 478, loss: 0.00059, acc: 0.99905\n",
      "global step 2850, epoch: 4, batch: 528, loss: 0.00026, acc: 0.99905\n",
      "global step 2900, epoch: 4, batch: 578, loss: 0.00143, acc: 0.99900\n",
      "global step 2950, epoch: 4, batch: 628, loss: 0.00055, acc: 0.99893\n",
      "global step 3000, epoch: 4, batch: 678, loss: 0.00325, acc: 0.99892\n",
      "global step 3050, epoch: 4, batch: 728, loss: 0.00054, acc: 0.99893\n",
      "eval loss: 0.01633, accu: 0.99564\n",
      "global step 3100, epoch: 5, batch: 4, loss: 0.00082, acc: 1.00000\n",
      "global step 3150, epoch: 5, batch: 54, loss: 0.13586, acc: 0.99884\n",
      "global step 3200, epoch: 5, batch: 104, loss: 0.00223, acc: 0.99895\n",
      "global step 3250, epoch: 5, batch: 154, loss: 0.00036, acc: 0.99899\n",
      "global step 3300, epoch: 5, batch: 204, loss: 0.00042, acc: 0.99893\n",
      "global step 3350, epoch: 5, batch: 254, loss: 0.00198, acc: 0.99895\n",
      "global step 3400, epoch: 5, batch: 304, loss: 0.00272, acc: 0.99907\n",
      "global step 3450, epoch: 5, batch: 354, loss: 0.00027, acc: 0.99921\n",
      "global step 3500, epoch: 5, batch: 404, loss: 0.00082, acc: 0.99927\n",
      "global step 3550, epoch: 5, batch: 454, loss: 0.00543, acc: 0.99911\n",
      "global step 3600, epoch: 5, batch: 504, loss: 0.00146, acc: 0.99913\n",
      "global step 3650, epoch: 5, batch: 554, loss: 0.00030, acc: 0.99913\n",
      "global step 3700, epoch: 5, batch: 604, loss: 0.00086, acc: 0.99912\n",
      "global step 3750, epoch: 5, batch: 654, loss: 0.00034, acc: 0.99916\n",
      "global step 3800, epoch: 5, batch: 704, loss: 0.00019, acc: 0.99922\n",
      "global step 3850, epoch: 5, batch: 754, loss: 0.00107, acc: 0.99915\n",
      "eval loss: 0.01511, accu: 0.99596\n",
      "global step 3900, epoch: 6, batch: 30, loss: 0.00251, acc: 1.00000\n",
      "global step 3950, epoch: 6, batch: 80, loss: 0.00068, acc: 1.00000\n",
      "global step 4000, epoch: 6, batch: 130, loss: 0.00057, acc: 0.99988\n",
      "global step 4050, epoch: 6, batch: 180, loss: 0.00016, acc: 0.99991\n",
      "global step 4100, epoch: 6, batch: 230, loss: 0.00041, acc: 0.99980\n",
      "global step 4150, epoch: 6, batch: 280, loss: 0.00669, acc: 0.99983\n",
      "global step 4200, epoch: 6, batch: 330, loss: 0.00018, acc: 0.99981\n",
      "global step 4250, epoch: 6, batch: 380, loss: 0.00019, acc: 0.99984\n",
      "global step 4300, epoch: 6, batch: 430, loss: 0.00016, acc: 0.99985\n",
      "global step 4350, epoch: 6, batch: 480, loss: 0.00035, acc: 0.99984\n",
      "global step 4400, epoch: 6, batch: 530, loss: 0.00026, acc: 0.99985\n",
      "global step 4450, epoch: 6, batch: 580, loss: 0.00017, acc: 0.99987\n",
      "global step 4500, epoch: 6, batch: 630, loss: 0.00073, acc: 0.99988\n",
      "global step 4550, epoch: 6, batch: 680, loss: 0.00051, acc: 0.99989\n",
      "global step 4600, epoch: 6, batch: 730, loss: 0.00039, acc: 0.99989\n",
      "eval loss: 0.01564, accu: 0.99644\n",
      "global step 4650, epoch: 7, batch: 6, loss: 0.00009, acc: 0.99740\n",
      "global step 4700, epoch: 7, batch: 56, loss: 0.00332, acc: 0.99916\n",
      "global step 4750, epoch: 7, batch: 106, loss: 0.00014, acc: 0.99941\n",
      "global step 4800, epoch: 7, batch: 156, loss: 0.00020, acc: 0.99940\n",
      "global step 4850, epoch: 7, batch: 206, loss: 0.00063, acc: 0.99947\n",
      "global step 4900, epoch: 7, batch: 256, loss: 0.00019, acc: 0.99957\n",
      "global step 4950, epoch: 7, batch: 306, loss: 0.00020, acc: 0.99954\n",
      "global step 5000, epoch: 7, batch: 356, loss: 0.00141, acc: 0.99960\n",
      "global step 5050, epoch: 7, batch: 406, loss: 0.00020, acc: 0.99965\n",
      "global step 5100, epoch: 7, batch: 456, loss: 0.00015, acc: 0.99955\n",
      "global step 5150, epoch: 7, batch: 506, loss: 0.00011, acc: 0.99960\n",
      "global step 5200, epoch: 7, batch: 556, loss: 0.00017, acc: 0.99961\n",
      "global step 5250, epoch: 7, batch: 606, loss: 0.00016, acc: 0.99961\n",
      "global step 5300, epoch: 7, batch: 656, loss: 0.00017, acc: 0.99964\n",
      "global step 5350, epoch: 7, batch: 706, loss: 0.00013, acc: 0.99960\n",
      "global step 5400, epoch: 7, batch: 756, loss: 0.00021, acc: 0.99959\n",
      "eval loss: 0.01574, accu: 0.99661\n",
      "global step 5450, epoch: 8, batch: 32, loss: 0.00011, acc: 1.00000\n",
      "global step 5500, epoch: 8, batch: 82, loss: 0.00028, acc: 0.99981\n",
      "global step 5550, epoch: 8, batch: 132, loss: 0.00012, acc: 0.99976\n",
      "global step 5600, epoch: 8, batch: 182, loss: 0.00009, acc: 0.99974\n",
      "global step 5650, epoch: 8, batch: 232, loss: 0.00007, acc: 0.99973\n",
      "global step 5700, epoch: 8, batch: 282, loss: 0.00010, acc: 0.99978\n",
      "global step 5750, epoch: 8, batch: 332, loss: 0.00012, acc: 0.99981\n",
      "global step 5800, epoch: 8, batch: 382, loss: 0.00012, acc: 0.99984\n",
      "global step 5850, epoch: 8, batch: 432, loss: 0.00011, acc: 0.99986\n",
      "global step 5900, epoch: 8, batch: 482, loss: 0.00040, acc: 0.99984\n",
      "global step 5950, epoch: 8, batch: 532, loss: 0.00010, acc: 0.99985\n",
      "global step 6000, epoch: 8, batch: 582, loss: 0.00021, acc: 0.99987\n",
      "global step 6050, epoch: 8, batch: 632, loss: 0.00012, acc: 0.99988\n",
      "global step 6100, epoch: 8, batch: 682, loss: 0.00011, acc: 0.99989\n",
      "global step 6150, epoch: 8, batch: 732, loss: 0.00111, acc: 0.99989\n",
      "eval loss: 0.01374, accu: 0.99774\n",
      "global step 6200, epoch: 9, batch: 8, loss: 0.00017, acc: 0.99805\n",
      "global step 6250, epoch: 9, batch: 58, loss: 0.00007, acc: 0.99973\n",
      "global step 6300, epoch: 9, batch: 108, loss: 0.00006, acc: 0.99986\n",
      "global step 6350, epoch: 9, batch: 158, loss: 0.00012, acc: 0.99990\n",
      "global step 6400, epoch: 9, batch: 208, loss: 0.00022, acc: 0.99992\n",
      "global step 6450, epoch: 9, batch: 258, loss: 0.00012, acc: 0.99994\n",
      "global step 6500, epoch: 9, batch: 308, loss: 0.00009, acc: 0.99995\n",
      "global step 6550, epoch: 9, batch: 358, loss: 0.00019, acc: 0.99996\n",
      "global step 6600, epoch: 9, batch: 408, loss: 0.00009, acc: 0.99996\n",
      "global step 6650, epoch: 9, batch: 458, loss: 0.00048, acc: 0.99993\n",
      "global step 6700, epoch: 9, batch: 508, loss: 0.00182, acc: 0.99991\n",
      "global step 6750, epoch: 9, batch: 558, loss: 0.00009, acc: 0.99992\n",
      "global step 6800, epoch: 9, batch: 608, loss: 0.00009, acc: 0.99992\n",
      "global step 6850, epoch: 9, batch: 658, loss: 0.00015, acc: 0.99991\n",
      "global step 6900, epoch: 9, batch: 708, loss: 0.00006, acc: 0.99989\n",
      "global step 6950, epoch: 9, batch: 758, loss: 0.00009, acc: 0.99990\n",
      "eval loss: 0.01519, accu: 0.99709\n",
      "global step 7000, epoch: 10, batch: 34, loss: 0.00010, acc: 0.99954\n",
      "global step 7050, epoch: 10, batch: 84, loss: 0.00020, acc: 0.99981\n",
      "global step 7100, epoch: 10, batch: 134, loss: 0.00008, acc: 0.99988\n",
      "global step 7150, epoch: 10, batch: 184, loss: 0.00106, acc: 0.99992\n",
      "global step 7200, epoch: 10, batch: 234, loss: 0.00023, acc: 0.99993\n",
      "global step 7250, epoch: 10, batch: 284, loss: 0.00008, acc: 0.99994\n",
      "global step 7300, epoch: 10, batch: 334, loss: 0.00007, acc: 0.99995\n",
      "global step 7350, epoch: 10, batch: 384, loss: 0.00005, acc: 0.99996\n",
      "global step 7400, epoch: 10, batch: 434, loss: 0.00012, acc: 0.99996\n",
      "global step 7450, epoch: 10, batch: 484, loss: 0.00006, acc: 0.99997\n",
      "\n",
      "global step 7550, epoch: 10, batch: 584, loss: 0.00007, acc: 0.99997\n",
      "global step 7600, epoch: 10, batch: 634, loss: 0.00007, acc: 0.99998\n",
      "global step 7650, epoch: 10, batch: 684, loss: 0.00006, acc: 0.99998\n",
      "global step 7700, epoch: 10, batch: 734, loss: 0.00010, acc: 0.99998\n",
      "eval loss: 0.01551, accu: 0.99725\n"
     ]
    }
   ],
   "source": [
    "#开始训练\r\n",
    "global_step = 0\r\n",
    "with LogWriter(logdir=\"./log\") as writer:\r\n",
    "    for epoch in range(1, epochs + 1):    \r\n",
    "        for step, batch in enumerate(train_loader, start=1): #从训练数据迭代器中取数据\r\n",
    "            input_ids, segment_ids, labels = batch\r\n",
    "            logits = model(input_ids, segment_ids)\r\n",
    "            loss = criterion(logits, labels) #计算损失\r\n",
    "            probs = F.softmax(logits, axis=1)\r\n",
    "            correct = metric.compute(probs, labels)\r\n",
    "            metric.update(correct)\r\n",
    "            acc = metric.accumulate()\r\n",
    "\r\n",
    "            global_step += 1\r\n",
    "            if global_step % 50 == 0 :\r\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\r\n",
    "                #记录训练过程\r\n",
    "                writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\r\n",
    "                writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            lr_scheduler.step()\r\n",
    "            optimizer.clear_gradients()\r\n",
    "        eval_loss, eval_acc = evaluate(model, criterion, metric, dev_loader)\r\n",
    "        #记录评估过程\r\n",
    "        writer.add_scalar(tag=\"eval/loss\", step=epoch, value=eval_loss)\r\n",
    "        writer.add_scalar(tag=\"eval/acc\", step=epoch, value=eval_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用visualdl可视化工具对模型训练的时候进行可视化操作，通过观察其所需时间，算法的收敛速度，以及其准确率的大小，方便和后面所使用的roberta模型进行对比\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/db856685ef134cd59e15e402b5d084ecc2fb0e43ece94bdca263c50808d84cbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, data, tokenizer, label_map, batch_size=1):\r\n",
    "    \"\"\"\r\n",
    "    Predicts the data labels.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        model (obj:`paddle.nn.Layer`): A model to classify texts.\r\n",
    "        data (obj:`List(Example)`): The processed data whose each element is a Example (numedtuple) object.\r\n",
    "            A Example object contains `text`(word_ids) and `se_len`(sequence length).\r\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \r\n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\r\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\r\n",
    "        batch_size(obj:`int`, defaults to 1): The number of batch.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        results(obj:`dict`): All the predictions labels.\r\n",
    "    \"\"\"\r\n",
    "    examples = []\r\n",
    "    for text in data:\r\n",
    "        input_ids, segment_ids = convert_example(\r\n",
    "            text,\r\n",
    "            tokenizer,\r\n",
    "            max_seq_length=128,\r\n",
    "            is_test=True)\r\n",
    "        examples.append((input_ids, segment_ids))\r\n",
    "\r\n",
    "    batchify_fn = lambda samples, fn=Tuple(\r\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\r\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\r\n",
    "    ): fn(samples)\r\n",
    "\r\n",
    "    # Seperates data into some batches.\r\n",
    "    batches = []\r\n",
    "    one_batch = []\r\n",
    "    for example in examples:\r\n",
    "        one_batch.append(example)\r\n",
    "        if len(one_batch) == batch_size:\r\n",
    "            batches.append(one_batch)\r\n",
    "            one_batch = []\r\n",
    "    if one_batch:\r\n",
    "        # The last batch whose size is less than the config batch_size setting.\r\n",
    "        batches.append(one_batch)\r\n",
    "\r\n",
    "    results = []\r\n",
    "    model.eval()\r\n",
    "    for batch in batches:\r\n",
    "        input_ids, segment_ids = batchify_fn(batch)\r\n",
    "        input_ids = paddle.to_tensor(input_ids)\r\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "        idx = idx.tolist()\r\n",
    "        labels = [label_map[i] for i in idx]\r\n",
    "        results.extend(labels)\r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测内容: {'text': '您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺'} \n",
      "邮件标签: 垃圾邮件\n"
     ]
    }
   ],
   "source": [
    "data = [{'text':'您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺'}]\r\n",
    "label_map = {0: '垃圾邮件', 1: '正常邮件'}\r\n",
    "\r\n",
    "predictions = predict(model, data, tokenizer_bert, label_map, batch_size=32)\r\n",
    "for idx, text in enumerate(data):\r\n",
    "    print('预测内容: {} \\n邮件标签: {}'.format(text, predictions[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2roberta模型\n",
    "### 3.2.1 roberta模型是什么\n",
    "[roberta模型论文](https://arxiv.org/abs/1907.11692)可以在这里下载到roberta算法的论文，同时[roberta算法](https://github.com/brightmart/roberta_zh)在github上已经有了开源的仓库\n",
    "\n",
    "roberta是bert的改进版，通过改进训练任务和数据生成方式、训练更久、使用更大批次、使用更多数据等获得了SOTA的效果\n",
    "\n",
    "roberta算法的改进如下\n",
    "- More data(更多的数据)\n",
    "> 文章基于 BERT 提出了一种效果更好的预训练模型训练方式，其主要的区别如下： 训练数据上，RoBERTa 采用了 160G 的训练文本，而 BERT 仅使用 16G 的训练文本。\n",
    "\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/eb138f1e814e4707b5d1cda1fa5c5b8a17e6d162cdb949f1bed00c901dd720c1\" width=\"  \"></div>\n",
    "<center>不同算法预训练数据量对比</center>\n",
    "\n",
    "- More Steps(更多训练)\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/876bd97d28d54aefae52f234a3a074537f3e7373c7e04c24a3a9ec5828183976\" width=\"  \"></div>\n",
    "\n",
    "- Large Batch(更大批次)\n",
    "> 批量（batch），常规设置128，256等等便可，如 BERT则是256，RoBERTa 在训练过程中使用了更大的批数量。研究人员尝试过从 256 到 8000 不等的批数量。\n",
    "\n",
    "- Adam optimizer\n",
    "\n",
    "Adam借鉴了Kingma等人的改进，使用$\\beta_1=0.9$、$\\beta_2=0.999$、$\\epsilon=1e-6$,并且$L_2$的衰减权重设置为$0.01$，在前10000$steps$是warmed up学习率是$1e-4$,并且是线性的衰减，所有层和Attention权重的dropout=0.1，预训练模型训练1,000,000steps最小batch256，最大batch512\n",
    "\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/da42f0ccea414bfcbb8be6d7362514586623311774d74417a78553816a4fd004\" width=\"  \"></div>\n",
    "\n",
    "<center>Transformer使用的warmed up学习率</center>\n",
    "\n",
    "- Next Sentence Prediction\n",
    "> Next Sentence Prediction (NSP) 数据生成方式和任务改进：取消下一个句子预测，并且数据连续从一个文档中获得\n",
    "### 3.2.2roberta模型结构\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/cf165061c4164fcaa6599e04501e44e2d9e43cc9c1834340ba8b8d9f5ffb061f\" width=\"  \"></div>\n",
    "### 3.2.3roberta模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-02-27 16:20:20,291] [    INFO]\u001b[0m - Already cached /home/aistudio/.paddlenlp/models/rbt3/rbt3_chn_large.pdparams\u001b[0m\n",
      "W0227 16:20:20.292970 17211 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0227 16:20:20.296628 17211 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "\u001b[32m[2022-02-27 16:20:31,473] [    INFO]\u001b[0m - Already cached /home/aistudio/.paddlenlp/models/rbt3/vocab.txt\u001b[0m\n",
      "global step 10, epoch: 1, batch: 10, loss: 0.45774, accu: 0.66563, speed: 8.24 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.39595, accu: 0.76406, speed: 15.31 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.24399, accu: 0.80000, speed: 15.78 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.29976, accu: 0.82734, speed: 15.67 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.11346, accu: 0.84688, speed: 15.47 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.45240, accu: 0.85938, speed: 15.18 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.15798, accu: 0.86786, speed: 14.73 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.24628, accu: 0.87344, speed: 8.41 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.12839, accu: 0.87847, speed: 14.46 step/s\n",
      "eval loss: 0.21342, accu: 0.92405\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.13951, accu: 0.89687, speed: 0.81 step/s\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.13137, accu: 0.90938, speed: 15.59 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.18262, accu: 0.91667, speed: 16.05 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.24581, accu: 0.91719, speed: 15.73 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.14215, accu: 0.92063, speed: 15.33 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.09420, accu: 0.92500, speed: 15.49 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.34941, accu: 0.92321, speed: 15.52 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.13866, accu: 0.92500, speed: 15.51 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.20291, accu: 0.92708, speed: 15.29 step/s\n",
      "eval loss: 0.17284, accu: 0.93811\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.06002, accu: 0.94063, speed: 0.81 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.17837, accu: 0.93281, speed: 15.30 step/s\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.19752, accu: 0.93229, speed: 15.13 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.20731, accu: 0.92891, speed: 15.93 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.05572, accu: 0.93188, speed: 15.39 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.37892, accu: 0.93177, speed: 14.40 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.11106, accu: 0.93259, speed: 15.78 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.39835, accu: 0.93242, speed: 16.11 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.25332, accu: 0.93021, speed: 15.60 step/s\n",
      "eval loss: 0.16541, accu: 0.94085\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.21534, accu: 0.91563, speed: 0.80 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.10694, accu: 0.92031, speed: 15.33 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.11118, accu: 0.92500, speed: 15.40 step/s\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.15366, accu: 0.93125, speed: 14.49 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.10862, accu: 0.93125, speed: 15.12 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.09213, accu: 0.93229, speed: 15.73 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.21474, accu: 0.93705, speed: 11.12 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.05046, accu: 0.93789, speed: 10.85 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.29757, accu: 0.93819, speed: 15.71 step/s\n",
      "eval loss: 0.15863, accu: 0.94085\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.12229, accu: 0.92812, speed: 0.80 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.12886, accu: 0.94063, speed: 15.61 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.30656, accu: 0.94479, speed: 15.67 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.15156, accu: 0.94922, speed: 15.52 step/s\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.06794, accu: 0.95000, speed: 15.82 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.10888, accu: 0.94948, speed: 15.44 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.07255, accu: 0.94911, speed: 15.30 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.10392, accu: 0.94883, speed: 14.99 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.08509, accu: 0.94792, speed: 15.31 step/s\n",
      "eval loss: 0.14151, accu: 0.95184\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.09894, accu: 0.94688, speed: 0.81 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.11538, accu: 0.95625, speed: 15.17 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.01644, accu: 0.94583, speed: 14.69 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.13190, accu: 0.94844, speed: 14.93 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.14364, accu: 0.94688, speed: 15.31 step/s\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.28241, accu: 0.94688, speed: 15.02 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.09836, accu: 0.94107, speed: 15.86 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.23409, accu: 0.94063, speed: 15.46 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.24818, accu: 0.94236, speed: 15.19 step/s\n",
      "eval loss: 0.13400, accu: 0.95168\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.15822, accu: 0.93437, speed: 0.82 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.24214, accu: 0.94375, speed: 13.61 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.05162, accu: 0.94688, speed: 15.52 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.06964, accu: 0.95078, speed: 15.76 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.14868, accu: 0.95000, speed: 15.89 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.14604, accu: 0.95104, speed: 15.61 step/s\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.04463, accu: 0.95134, speed: 15.53 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.10038, accu: 0.95039, speed: 15.57 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.08492, accu: 0.95174, speed: 10.63 step/s\n",
      "eval loss: 0.13382, accu: 0.95637\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.18454, accu: 0.96875, speed: 0.79 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.10782, accu: 0.96406, speed: 15.99 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.04257, accu: 0.96146, speed: 14.79 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.08230, accu: 0.95625, speed: 15.89 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.17695, accu: 0.95312, speed: 15.46 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.39099, accu: 0.94948, speed: 16.29 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.17198, accu: 0.95134, speed: 15.75 step/s\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.03399, accu: 0.95352, speed: 14.77 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.05808, accu: 0.95625, speed: 14.77 step/s\n",
      "eval loss: 0.14692, accu: 0.95297\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.24195, accu: 0.94063, speed: 0.80 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.08553, accu: 0.94844, speed: 15.17 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.19507, accu: 0.94583, speed: 15.58 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.15844, accu: 0.94531, speed: 14.44 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.03763, accu: 0.94875, speed: 15.17 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.13507, accu: 0.94427, speed: 14.66 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.15419, accu: 0.94464, speed: 15.02 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.05552, accu: 0.94727, speed: 15.23 step/s\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.10103, accu: 0.94722, speed: 16.05 step/s\n",
      "eval loss: 0.11909, accu: 0.95863\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.20178, accu: 0.95625, speed: 0.82 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.07899, accu: 0.95781, speed: 15.55 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.12545, accu: 0.96042, speed: 15.63 step/s\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.10780, accu: 0.96250, speed: 15.40 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.04882, accu: 0.95937, speed: 15.56 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.12286, accu: 0.95990, speed: 15.44 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.32622, accu: 0.95937, speed: 15.09 step/s\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.02195, accu: 0.96016, speed: 15.40 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.02220, accu: 0.96111, speed: 10.17 step/s\n",
      "eval loss: 0.11889, accu: 0.96202\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.38991, accu: 0.92188, speed: 0.81 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.10073, accu: 0.94375, speed: 15.32 step/s\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.04366, accu: 0.95208, speed: 15.56 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.02566, accu: 0.96016, speed: 16.18 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.02081, accu: 0.96000, speed: 15.64 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.11251, accu: 0.96146, speed: 15.90 step/s\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.17205, accu: 0.96027, speed: 15.07 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.16855, accu: 0.96133, speed: 15.79 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.11462, accu: 0.96285, speed: 15.50 step/s\n",
      "eval loss: 0.11300, accu: 0.96315\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.10784, accu: 0.95312, speed: 0.82 step/s\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.06673, accu: 0.95937, speed: 15.44 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.10732, accu: 0.95937, speed: 16.24 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.11806, accu: 0.96172, speed: 15.17 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.11594, accu: 0.96000, speed: 14.21 step/s\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.05142, accu: 0.95885, speed: 15.87 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.06729, accu: 0.95714, speed: 15.81 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.02779, accu: 0.95820, speed: 16.19 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.12941, accu: 0.95660, speed: 16.01 step/s\n",
      "eval loss: 0.12070, accu: 0.95427\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.09527, accu: 0.95625, speed: 0.79 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.08695, accu: 0.96875, speed: 16.14 step/s\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.26259, accu: 0.96458, speed: 15.77 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.13139, accu: 0.96797, speed: 16.75 step/s\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.02092, accu: 0.96625, speed: 16.09 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.02423, accu: 0.96719, speed: 14.59 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.07552, accu: 0.96607, speed: 14.22 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.03768, accu: 0.96641, speed: 15.05 step/s\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.30917, accu: 0.96389, speed: 8.18 step/s\n",
      "eval loss: 0.09816, accu: 0.96574\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.32613, accu: 0.96875, speed: 0.77 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.10835, accu: 0.96562, speed: 15.47 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.16858, accu: 0.96354, speed: 15.50 step/s\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.05965, accu: 0.96484, speed: 15.62 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.06071, accu: 0.96688, speed: 15.69 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.12157, accu: 0.96458, speed: 15.80 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.03146, accu: 0.96473, speed: 15.64 step/s\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.13242, accu: 0.96406, speed: 16.19 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.16697, accu: 0.96215, speed: 15.25 step/s\n",
      "eval loss: 0.10465, accu: 0.96396\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.15864, accu: 0.96562, speed: 0.81 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.12854, accu: 0.95469, speed: 15.76 step/s\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.04275, accu: 0.95521, speed: 15.57 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.13862, accu: 0.95469, speed: 14.51 step/s\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.03426, accu: 0.96062, speed: 15.51 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.03373, accu: 0.96198, speed: 15.23 step/s\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.09028, accu: 0.96384, speed: 16.04 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.17115, accu: 0.96406, speed: 15.56 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.07879, accu: 0.96458, speed: 15.29 step/s\n",
      "eval loss: 0.09249, accu: 0.96784\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.11992, accu: 0.98438, speed: 0.82 step/s\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.02907, accu: 0.97656, speed: 15.97 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.01637, accu: 0.97396, speed: 16.20 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.04857, accu: 0.97266, speed: 15.91 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.11318, accu: 0.97125, speed: 15.73 step/s\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.03106, accu: 0.97292, speed: 15.29 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.12902, accu: 0.97321, speed: 15.18 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.09792, accu: 0.97305, speed: 15.80 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.14428, accu: 0.97188, speed: 11.16 step/s\n",
      "eval loss: 0.09930, accu: 0.96574\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.09394, accu: 0.95937, speed: 0.80 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.05391, accu: 0.96562, speed: 15.21 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.20375, accu: 0.95833, speed: 15.70 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.32233, accu: 0.95625, speed: 15.59 step/s\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.03343, accu: 0.95813, speed: 15.31 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.06570, accu: 0.96094, speed: 15.78 step/s\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.23563, accu: 0.96027, speed: 15.39 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.01303, accu: 0.96094, speed: 15.67 step/s\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.15746, accu: 0.96042, speed: 15.76 step/s\n",
      "eval loss: 0.09396, accu: 0.96800\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.06495, accu: 0.94688, speed: 0.81 step/s\n",
      "global step 1550, epoch: 2, batch: 2, loss: 0.12225, accu: 0.94127, speed: 14.78 step/s\n",
      "global step 1560, epoch: 2, batch: 12, loss: 0.10673, accu: 0.95391, speed: 14.83 step/s\n",
      "global step 1570, epoch: 2, batch: 22, loss: 0.02169, accu: 0.96010, speed: 14.20 step/s\n",
      "global step 1580, epoch: 2, batch: 32, loss: 0.02307, accu: 0.96758, speed: 15.23 step/s\n",
      "global step 1590, epoch: 2, batch: 42, loss: 0.02248, accu: 0.97042, speed: 13.89 step/s\n",
      "global step 1600, epoch: 2, batch: 52, loss: 0.01755, accu: 0.97063, speed: 14.11 step/s\n",
      "global step 1610, epoch: 2, batch: 62, loss: 0.00382, accu: 0.97276, speed: 15.61 step/s\n",
      "global step 1620, epoch: 2, batch: 72, loss: 0.15593, accu: 0.97336, speed: 15.62 step/s\n",
      "eval loss: 0.13008, accu: 0.96186\n",
      "global step 1630, epoch: 2, batch: 82, loss: 0.09089, accu: 0.98438, speed: 0.81 step/s\n",
      "global step 1640, epoch: 2, batch: 92, loss: 0.05982, accu: 0.97656, speed: 16.07 step/s\n",
      "global step 1650, epoch: 2, batch: 102, loss: 0.15694, accu: 0.97604, speed: 15.71 step/s\n",
      "global step 1660, epoch: 2, batch: 112, loss: 0.02860, accu: 0.97891, speed: 16.23 step/s\n",
      "global step 1670, epoch: 2, batch: 122, loss: 0.08694, accu: 0.97813, speed: 14.72 step/s\n",
      "global step 1680, epoch: 2, batch: 132, loss: 0.01321, accu: 0.97656, speed: 13.50 step/s\n",
      "global step 1690, epoch: 2, batch: 142, loss: 0.00697, accu: 0.97679, speed: 14.24 step/s\n",
      "global step 1700, epoch: 2, batch: 152, loss: 0.04496, accu: 0.97813, speed: 15.64 step/s\n",
      "global step 1710, epoch: 2, batch: 162, loss: 0.02426, accu: 0.97882, speed: 12.30 step/s\n",
      "eval loss: 0.09130, accu: 0.97269\n",
      "global step 1720, epoch: 2, batch: 172, loss: 0.10458, accu: 0.96875, speed: 0.79 step/s\n",
      "global step 1730, epoch: 2, batch: 182, loss: 0.00825, accu: 0.96406, speed: 15.57 step/s\n",
      "global step 1740, epoch: 2, batch: 192, loss: 0.04364, accu: 0.97188, speed: 16.04 step/s\n",
      "global step 1750, epoch: 2, batch: 202, loss: 0.03204, accu: 0.97344, speed: 15.74 step/s\n",
      "global step 1760, epoch: 2, batch: 212, loss: 0.00827, accu: 0.97562, speed: 15.45 step/s\n",
      "global step 1770, epoch: 2, batch: 222, loss: 0.02510, accu: 0.97500, speed: 15.65 step/s\n",
      "global step 1780, epoch: 2, batch: 232, loss: 0.01012, accu: 0.97589, speed: 15.65 step/s\n",
      "global step 1790, epoch: 2, batch: 242, loss: 0.02976, accu: 0.97617, speed: 15.55 step/s\n",
      "global step 1800, epoch: 2, batch: 252, loss: 0.01629, accu: 0.97743, speed: 16.27 step/s\n",
      "eval loss: 0.08995, accu: 0.97156\n",
      "global step 1810, epoch: 2, batch: 262, loss: 0.02193, accu: 0.98438, speed: 0.81 step/s\n",
      "global step 1820, epoch: 2, batch: 272, loss: 0.00960, accu: 0.98438, speed: 15.56 step/s\n",
      "global step 1830, epoch: 2, batch: 282, loss: 0.00703, accu: 0.98333, speed: 14.81 step/s\n",
      "global step 1840, epoch: 2, batch: 292, loss: 0.01320, accu: 0.98516, speed: 15.59 step/s\n",
      "global step 1850, epoch: 2, batch: 302, loss: 0.07646, accu: 0.98062, speed: 15.12 step/s\n",
      "global step 1860, epoch: 2, batch: 312, loss: 0.01563, accu: 0.98125, speed: 15.08 step/s\n",
      "global step 1870, epoch: 2, batch: 322, loss: 0.02491, accu: 0.98080, speed: 15.39 step/s\n",
      "global step 1880, epoch: 2, batch: 332, loss: 0.01248, accu: 0.98047, speed: 15.22 step/s\n",
      "global step 1890, epoch: 2, batch: 342, loss: 0.04879, accu: 0.98021, speed: 15.50 step/s\n",
      "eval loss: 0.08848, accu: 0.97172\n",
      "global step 1900, epoch: 2, batch: 352, loss: 0.01834, accu: 0.97813, speed: 0.83 step/s\n",
      "global step 1910, epoch: 2, batch: 362, loss: 0.02450, accu: 0.97969, speed: 15.47 step/s\n",
      "global step 1920, epoch: 2, batch: 372, loss: 0.03084, accu: 0.98229, speed: 15.59 step/s\n",
      "global step 1930, epoch: 2, batch: 382, loss: 0.02046, accu: 0.97969, speed: 15.52 step/s\n",
      "global step 1940, epoch: 2, batch: 392, loss: 0.00438, accu: 0.98062, speed: 15.74 step/s\n",
      "global step 1950, epoch: 2, batch: 402, loss: 0.05215, accu: 0.98177, speed: 15.66 step/s\n",
      "global step 1960, epoch: 2, batch: 412, loss: 0.02107, accu: 0.98080, speed: 16.11 step/s\n",
      "global step 1970, epoch: 2, batch: 422, loss: 0.04854, accu: 0.98203, speed: 15.63 step/s\n",
      "global step 1980, epoch: 2, batch: 432, loss: 0.01781, accu: 0.98299, speed: 15.65 step/s\n",
      "eval loss: 0.10852, accu: 0.96768\n",
      "global step 1990, epoch: 2, batch: 442, loss: 0.10464, accu: 0.99375, speed: 0.79 step/s\n",
      "global step 2000, epoch: 2, batch: 452, loss: 0.19597, accu: 0.99062, speed: 14.51 step/s\n",
      "global step 2010, epoch: 2, batch: 462, loss: 0.05028, accu: 0.98958, speed: 15.20 step/s\n",
      "global step 2020, epoch: 2, batch: 472, loss: 0.00773, accu: 0.98516, speed: 14.03 step/s\n",
      "global step 2030, epoch: 2, batch: 482, loss: 0.00999, accu: 0.98562, speed: 14.44 step/s\n",
      "global step 2040, epoch: 2, batch: 492, loss: 0.03410, accu: 0.98333, speed: 15.51 step/s\n",
      "global step 2050, epoch: 2, batch: 502, loss: 0.01287, accu: 0.98259, speed: 15.87 step/s\n",
      "global step 2060, epoch: 2, batch: 512, loss: 0.12686, accu: 0.98125, speed: 14.83 step/s\n",
      "global step 2070, epoch: 2, batch: 522, loss: 0.03414, accu: 0.98090, speed: 15.47 step/s\n",
      "eval loss: 0.08484, accu: 0.97479\n",
      "global step 2080, epoch: 2, batch: 532, loss: 0.01734, accu: 0.99062, speed: 0.82 step/s\n",
      "global step 2090, epoch: 2, batch: 542, loss: 0.13629, accu: 0.97969, speed: 14.77 step/s\n",
      "global step 2100, epoch: 2, batch: 552, loss: 0.01389, accu: 0.98333, speed: 15.69 step/s\n",
      "global step 2110, epoch: 2, batch: 562, loss: 0.00379, accu: 0.98281, speed: 16.01 step/s\n",
      "global step 2120, epoch: 2, batch: 572, loss: 0.12508, accu: 0.98313, speed: 15.45 step/s\n",
      "global step 2130, epoch: 2, batch: 582, loss: 0.00847, accu: 0.98177, speed: 14.06 step/s\n",
      "global step 2140, epoch: 2, batch: 592, loss: 0.00756, accu: 0.98259, speed: 15.42 step/s\n",
      "global step 2150, epoch: 2, batch: 602, loss: 0.03532, accu: 0.98086, speed: 15.49 step/s\n",
      "global step 2160, epoch: 2, batch: 612, loss: 0.22352, accu: 0.98160, speed: 15.90 step/s\n",
      "eval loss: 0.08594, accu: 0.97237\n",
      "global step 2170, epoch: 2, batch: 622, loss: 0.22001, accu: 0.95937, speed: 0.81 step/s\n",
      "global step 2180, epoch: 2, batch: 632, loss: 0.05446, accu: 0.97031, speed: 15.60 step/s\n",
      "global step 2190, epoch: 2, batch: 642, loss: 0.00602, accu: 0.97917, speed: 15.94 step/s\n",
      "global step 2200, epoch: 2, batch: 652, loss: 0.01136, accu: 0.97656, speed: 15.09 step/s\n",
      "global step 2210, epoch: 2, batch: 662, loss: 0.13668, accu: 0.98000, speed: 15.20 step/s\n",
      "global step 2220, epoch: 2, batch: 672, loss: 0.00229, accu: 0.98177, speed: 15.82 step/s\n",
      "global step 2230, epoch: 2, batch: 682, loss: 0.16426, accu: 0.98304, speed: 15.73 step/s\n",
      "global step 2240, epoch: 2, batch: 692, loss: 0.00664, accu: 0.98320, speed: 15.62 step/s\n",
      "global step 2250, epoch: 2, batch: 702, loss: 0.03343, accu: 0.98299, speed: 15.71 step/s\n",
      "eval loss: 0.09091, accu: 0.97301\n",
      "global step 2260, epoch: 2, batch: 712, loss: 0.01778, accu: 0.98438, speed: 0.78 step/s\n",
      "global step 2270, epoch: 2, batch: 722, loss: 0.06113, accu: 0.98125, speed: 15.63 step/s\n",
      "global step 2280, epoch: 2, batch: 732, loss: 0.01016, accu: 0.98333, speed: 15.24 step/s\n",
      "global step 2290, epoch: 2, batch: 742, loss: 0.00758, accu: 0.98594, speed: 14.01 step/s\n",
      "global step 2300, epoch: 2, batch: 752, loss: 0.01357, accu: 0.98500, speed: 14.36 step/s\n",
      "global step 2310, epoch: 2, batch: 762, loss: 0.03087, accu: 0.98385, speed: 16.10 step/s\n",
      "global step 2320, epoch: 2, batch: 772, loss: 0.16980, accu: 0.98438, speed: 15.61 step/s\n",
      "global step 2330, epoch: 2, batch: 782, loss: 0.10141, accu: 0.98281, speed: 15.87 step/s\n",
      "global step 2340, epoch: 2, batch: 792, loss: 0.09155, accu: 0.98264, speed: 16.22 step/s\n",
      "eval loss: 0.08732, accu: 0.97140\n",
      "global step 2350, epoch: 2, batch: 802, loss: 0.02149, accu: 0.97813, speed: 0.78 step/s\n",
      "global step 2360, epoch: 2, batch: 812, loss: 0.11851, accu: 0.98125, speed: 16.09 step/s\n",
      "global step 2370, epoch: 2, batch: 822, loss: 0.00683, accu: 0.98021, speed: 15.49 step/s\n",
      "global step 2380, epoch: 2, batch: 832, loss: 0.02967, accu: 0.97891, speed: 15.21 step/s\n",
      "global step 2390, epoch: 2, batch: 842, loss: 0.06905, accu: 0.97937, speed: 15.76 step/s\n",
      "global step 2400, epoch: 2, batch: 852, loss: 0.00597, accu: 0.98073, speed: 14.95 step/s\n",
      "global step 2410, epoch: 2, batch: 862, loss: 0.05341, accu: 0.97857, speed: 15.78 step/s\n",
      "global step 2420, epoch: 2, batch: 872, loss: 0.00900, accu: 0.97891, speed: 15.78 step/s\n",
      "global step 2430, epoch: 2, batch: 882, loss: 0.00527, accu: 0.97986, speed: 15.80 step/s\n",
      "eval loss: 0.08673, accu: 0.97317\n",
      "global step 2440, epoch: 2, batch: 892, loss: 0.00902, accu: 0.98125, speed: 0.81 step/s\n",
      "global step 2450, epoch: 2, batch: 902, loss: 0.00780, accu: 0.98125, speed: 14.54 step/s\n",
      "global step 2460, epoch: 2, batch: 912, loss: 0.04112, accu: 0.97917, speed: 15.13 step/s\n",
      "global step 2470, epoch: 2, batch: 922, loss: 0.01721, accu: 0.97813, speed: 15.52 step/s\n",
      "global step 2480, epoch: 2, batch: 932, loss: 0.02013, accu: 0.98062, speed: 15.50 step/s\n",
      "global step 2490, epoch: 2, batch: 942, loss: 0.01276, accu: 0.98125, speed: 15.99 step/s\n",
      "global step 2500, epoch: 2, batch: 952, loss: 0.02798, accu: 0.98304, speed: 15.11 step/s\n",
      "global step 2510, epoch: 2, batch: 962, loss: 0.00608, accu: 0.98359, speed: 15.85 step/s\n",
      "global step 2520, epoch: 2, batch: 972, loss: 0.02461, accu: 0.98368, speed: 16.00 step/s\n",
      "eval loss: 0.09792, accu: 0.97091\n",
      "global step 2530, epoch: 2, batch: 982, loss: 0.14267, accu: 0.97813, speed: 0.78 step/s\n",
      "global step 2540, epoch: 2, batch: 992, loss: 0.00957, accu: 0.98438, speed: 15.66 step/s\n",
      "global step 2550, epoch: 2, batch: 1002, loss: 0.08132, accu: 0.98542, speed: 15.70 step/s\n",
      "global step 2560, epoch: 2, batch: 1012, loss: 0.03260, accu: 0.98516, speed: 15.42 step/s\n",
      "global step 2570, epoch: 2, batch: 1022, loss: 0.01248, accu: 0.98438, speed: 15.45 step/s\n",
      "global step 2580, epoch: 2, batch: 1032, loss: 0.11784, accu: 0.98438, speed: 15.82 step/s\n",
      "global step 2590, epoch: 2, batch: 1042, loss: 0.00684, accu: 0.98571, speed: 15.33 step/s\n",
      "global step 2600, epoch: 2, batch: 1052, loss: 0.01210, accu: 0.98594, speed: 15.07 step/s\n",
      "global step 2610, epoch: 2, batch: 1062, loss: 0.00376, accu: 0.98715, speed: 15.15 step/s\n",
      "eval loss: 0.07954, accu: 0.97738\n",
      "global step 2620, epoch: 2, batch: 1072, loss: 0.06297, accu: 0.97500, speed: 0.82 step/s\n",
      "global step 2630, epoch: 2, batch: 1082, loss: 0.00580, accu: 0.97656, speed: 15.68 step/s\n",
      "global step 2640, epoch: 2, batch: 1092, loss: 0.01600, accu: 0.98125, speed: 15.59 step/s\n",
      "global step 2650, epoch: 2, batch: 1102, loss: 0.09263, accu: 0.97969, speed: 15.63 step/s\n",
      "global step 2660, epoch: 2, batch: 1112, loss: 0.10650, accu: 0.98062, speed: 15.42 step/s\n",
      "global step 2670, epoch: 2, batch: 1122, loss: 0.05769, accu: 0.98177, speed: 15.89 step/s\n",
      "global step 2680, epoch: 2, batch: 1132, loss: 0.14597, accu: 0.98259, speed: 15.49 step/s\n",
      "global step 2690, epoch: 2, batch: 1142, loss: 0.08653, accu: 0.98359, speed: 15.47 step/s\n",
      "global step 2700, epoch: 2, batch: 1152, loss: 0.01303, accu: 0.98333, speed: 15.53 step/s\n",
      "eval loss: 0.08143, accu: 0.97576\n",
      "global step 2710, epoch: 2, batch: 1162, loss: 0.02008, accu: 0.97813, speed: 0.82 step/s\n",
      "global step 2720, epoch: 2, batch: 1172, loss: 0.00811, accu: 0.97969, speed: 15.52 step/s\n",
      "global step 2730, epoch: 2, batch: 1182, loss: 0.05816, accu: 0.98438, speed: 16.12 step/s\n",
      "global step 2740, epoch: 2, batch: 1192, loss: 0.00750, accu: 0.98359, speed: 15.42 step/s\n",
      "global step 2750, epoch: 2, batch: 1202, loss: 0.00736, accu: 0.98250, speed: 16.08 step/s\n",
      "global step 2760, epoch: 2, batch: 1212, loss: 0.12351, accu: 0.98385, speed: 15.34 step/s\n",
      "global step 2770, epoch: 2, batch: 1222, loss: 0.00186, accu: 0.98571, speed: 15.29 step/s\n",
      "global step 2780, epoch: 2, batch: 1232, loss: 0.10196, accu: 0.98594, speed: 15.69 step/s\n",
      "global step 2790, epoch: 2, batch: 1242, loss: 0.01123, accu: 0.98681, speed: 15.81 step/s\n",
      "eval loss: 0.08002, accu: 0.97899\n",
      "global step 2800, epoch: 2, batch: 1252, loss: 0.01011, accu: 0.97188, speed: 0.76 step/s\n",
      "global step 2810, epoch: 2, batch: 1262, loss: 0.03719, accu: 0.97969, speed: 15.77 step/s\n",
      "global step 2820, epoch: 2, batch: 1272, loss: 0.01230, accu: 0.98333, speed: 16.06 step/s\n",
      "global step 2830, epoch: 2, batch: 1282, loss: 0.19054, accu: 0.98281, speed: 15.11 step/s\n",
      "global step 2840, epoch: 2, batch: 1292, loss: 0.05300, accu: 0.97875, speed: 15.37 step/s\n",
      "global step 2850, epoch: 2, batch: 1302, loss: 0.03139, accu: 0.98073, speed: 15.59 step/s\n",
      "global step 2860, epoch: 2, batch: 1312, loss: 0.12933, accu: 0.98080, speed: 15.64 step/s\n",
      "global step 2870, epoch: 2, batch: 1322, loss: 0.00562, accu: 0.98086, speed: 15.95 step/s\n",
      "global step 2880, epoch: 2, batch: 1332, loss: 0.01126, accu: 0.97951, speed: 15.75 step/s\n",
      "eval loss: 0.07345, accu: 0.97786\n",
      "global step 2890, epoch: 2, batch: 1342, loss: 0.02737, accu: 0.97188, speed: 0.81 step/s\n",
      "global step 2900, epoch: 2, batch: 1352, loss: 0.03810, accu: 0.98281, speed: 15.31 step/s\n",
      "global step 2910, epoch: 2, batch: 1362, loss: 0.13271, accu: 0.98229, speed: 14.92 step/s\n",
      "global step 2920, epoch: 2, batch: 1372, loss: 0.02750, accu: 0.98203, speed: 15.88 step/s\n",
      "global step 2930, epoch: 2, batch: 1382, loss: 0.03792, accu: 0.97813, speed: 15.46 step/s\n",
      "global step 2940, epoch: 2, batch: 1392, loss: 0.01050, accu: 0.97865, speed: 15.85 step/s\n",
      "global step 2950, epoch: 2, batch: 1402, loss: 0.00730, accu: 0.97857, speed: 16.27 step/s\n",
      "global step 2960, epoch: 2, batch: 1412, loss: 0.01038, accu: 0.97734, speed: 15.51 step/s\n",
      "global step 2970, epoch: 2, batch: 1422, loss: 0.17346, accu: 0.97813, speed: 15.38 step/s\n",
      "eval loss: 0.07226, accu: 0.97867\n",
      "global step 2980, epoch: 2, batch: 1432, loss: 0.00561, accu: 0.98438, speed: 0.80 step/s\n",
      "global step 2990, epoch: 2, batch: 1442, loss: 0.00524, accu: 0.98281, speed: 15.58 step/s\n",
      "global step 3000, epoch: 2, batch: 1452, loss: 0.09783, accu: 0.97708, speed: 15.82 step/s\n",
      "global step 3010, epoch: 2, batch: 1462, loss: 0.00841, accu: 0.97734, speed: 15.64 step/s\n",
      "global step 3020, epoch: 2, batch: 1472, loss: 0.00420, accu: 0.97750, speed: 15.76 step/s\n",
      "global step 3030, epoch: 2, batch: 1482, loss: 0.01480, accu: 0.97969, speed: 14.93 step/s\n",
      "global step 3040, epoch: 2, batch: 1492, loss: 0.01240, accu: 0.98080, speed: 15.96 step/s\n",
      "global step 3050, epoch: 2, batch: 1502, loss: 0.00459, accu: 0.98086, speed: 15.12 step/s\n",
      "global step 3060, epoch: 2, batch: 1512, loss: 0.00286, accu: 0.98194, speed: 15.58 step/s\n",
      "eval loss: 0.08626, accu: 0.97576\n",
      "global step 3070, epoch: 2, batch: 1522, loss: 0.04707, accu: 0.97500, speed: 0.80 step/s\n",
      "global step 3080, epoch: 2, batch: 1532, loss: 0.06218, accu: 0.97969, speed: 15.64 step/s\n",
      "global step 3090, epoch: 2, batch: 1542, loss: 0.03825, accu: 0.98125, speed: 15.62 step/s\n",
      "global step 3100, epoch: 3, batch: 4, loss: 0.00123, accu: 0.98404, speed: 15.18 step/s\n",
      "global step 3110, epoch: 3, batch: 14, loss: 0.09491, accu: 0.98474, speed: 15.54 step/s\n",
      "global step 3120, epoch: 3, batch: 24, loss: 0.00888, accu: 0.98521, speed: 15.48 step/s\n",
      "global step 3130, epoch: 3, batch: 34, loss: 0.09722, accu: 0.98599, speed: 15.97 step/s\n",
      "global step 3140, epoch: 3, batch: 44, loss: 0.00142, accu: 0.98737, speed: 16.08 step/s\n",
      "global step 3150, epoch: 3, batch: 54, loss: 0.02063, accu: 0.98878, speed: 15.73 step/s\n",
      "eval loss: 0.07667, accu: 0.97931\n",
      "global step 3160, epoch: 3, batch: 64, loss: 0.03887, accu: 0.98750, speed: 0.81 step/s\n",
      "global step 3170, epoch: 3, batch: 74, loss: 0.00257, accu: 0.98750, speed: 15.24 step/s\n",
      "global step 3180, epoch: 3, batch: 84, loss: 0.00118, accu: 0.98958, speed: 15.49 step/s\n",
      "global step 3190, epoch: 3, batch: 94, loss: 0.01157, accu: 0.99062, speed: 15.65 step/s\n",
      "global step 3200, epoch: 3, batch: 104, loss: 0.00094, accu: 0.99187, speed: 15.22 step/s\n",
      "global step 3210, epoch: 3, batch: 114, loss: 0.09333, accu: 0.99115, speed: 15.34 step/s\n",
      "global step 3220, epoch: 3, batch: 124, loss: 0.00249, accu: 0.99196, speed: 15.75 step/s\n",
      "global step 3230, epoch: 3, batch: 134, loss: 0.20137, accu: 0.99180, speed: 15.71 step/s\n",
      "global step 3240, epoch: 3, batch: 144, loss: 0.00105, accu: 0.99167, speed: 15.71 step/s\n",
      "eval loss: 0.08180, accu: 0.97867\n",
      "global step 3250, epoch: 3, batch: 154, loss: 0.01373, accu: 0.99687, speed: 0.81 step/s\n",
      "global step 3260, epoch: 3, batch: 164, loss: 0.00127, accu: 0.99375, speed: 15.45 step/s\n",
      "global step 3270, epoch: 3, batch: 174, loss: 0.01198, accu: 0.99271, speed: 15.73 step/s\n",
      "global step 3280, epoch: 3, batch: 184, loss: 0.00044, accu: 0.99141, speed: 15.93 step/s\n",
      "global step 3290, epoch: 3, batch: 194, loss: 0.01987, accu: 0.99313, speed: 15.49 step/s\n",
      "global step 3300, epoch: 3, batch: 204, loss: 0.00044, accu: 0.99323, speed: 14.97 step/s\n",
      "global step 3310, epoch: 3, batch: 214, loss: 0.00119, accu: 0.99330, speed: 15.72 step/s\n",
      "global step 3320, epoch: 3, batch: 224, loss: 0.01972, accu: 0.99336, speed: 15.15 step/s\n",
      "global step 3330, epoch: 3, batch: 234, loss: 0.04910, accu: 0.99201, speed: 15.02 step/s\n",
      "eval loss: 0.08783, accu: 0.97899\n",
      "global step 3340, epoch: 3, batch: 244, loss: 0.01403, accu: 0.99375, speed: 0.77 step/s\n",
      "global step 3350, epoch: 3, batch: 254, loss: 0.06281, accu: 0.99062, speed: 15.39 step/s\n",
      "global step 3360, epoch: 3, batch: 264, loss: 0.00184, accu: 0.99271, speed: 15.62 step/s\n",
      "global step 3370, epoch: 3, batch: 274, loss: 0.00160, accu: 0.99219, speed: 15.84 step/s\n",
      "global step 3380, epoch: 3, batch: 284, loss: 0.18043, accu: 0.98938, speed: 15.71 step/s\n",
      "global step 3390, epoch: 3, batch: 294, loss: 0.00509, accu: 0.99062, speed: 15.27 step/s\n",
      "global step 3400, epoch: 3, batch: 304, loss: 0.00356, accu: 0.99152, speed: 15.14 step/s\n",
      "global step 3410, epoch: 3, batch: 314, loss: 0.00567, accu: 0.99258, speed: 15.44 step/s\n",
      "global step 3420, epoch: 3, batch: 324, loss: 0.00411, accu: 0.99306, speed: 14.90 step/s\n",
      "eval loss: 0.09347, accu: 0.97883\n",
      "global step 3430, epoch: 3, batch: 334, loss: 0.01398, accu: 0.99062, speed: 0.81 step/s\n",
      "global step 3440, epoch: 3, batch: 344, loss: 0.08408, accu: 0.99219, speed: 15.88 step/s\n",
      "global step 3450, epoch: 3, batch: 354, loss: 0.02480, accu: 0.99062, speed: 15.91 step/s\n",
      "global step 3460, epoch: 3, batch: 364, loss: 0.01125, accu: 0.99141, speed: 15.49 step/s\n",
      "global step 3470, epoch: 3, batch: 374, loss: 0.00283, accu: 0.99125, speed: 15.34 step/s\n",
      "global step 3480, epoch: 3, batch: 384, loss: 0.00190, accu: 0.99062, speed: 15.77 step/s\n",
      "global step 3490, epoch: 3, batch: 394, loss: 0.00944, accu: 0.99152, speed: 15.33 step/s\n",
      "global step 3500, epoch: 3, batch: 404, loss: 0.01314, accu: 0.99062, speed: 15.70 step/s\n",
      "global step 3510, epoch: 3, batch: 414, loss: 0.00345, accu: 0.99132, speed: 15.60 step/s\n",
      "eval loss: 0.08623, accu: 0.97867\n",
      "global step 3520, epoch: 3, batch: 424, loss: 0.06096, accu: 0.99687, speed: 0.80 step/s\n",
      "global step 3530, epoch: 3, batch: 434, loss: 0.00213, accu: 0.99531, speed: 15.85 step/s\n",
      "global step 3540, epoch: 3, batch: 444, loss: 0.00380, accu: 0.99583, speed: 15.67 step/s\n",
      "global step 3550, epoch: 3, batch: 454, loss: 0.00073, accu: 0.99531, speed: 15.46 step/s\n",
      "global step 3560, epoch: 3, batch: 464, loss: 0.02713, accu: 0.99375, speed: 15.70 step/s\n",
      "global step 3570, epoch: 3, batch: 474, loss: 0.01264, accu: 0.99323, speed: 15.52 step/s\n",
      "global step 3580, epoch: 3, batch: 484, loss: 0.06994, accu: 0.99375, speed: 15.75 step/s\n",
      "global step 3590, epoch: 3, batch: 494, loss: 0.00565, accu: 0.99336, speed: 15.25 step/s\n",
      "global step 3600, epoch: 3, batch: 504, loss: 0.00483, accu: 0.99306, speed: 15.10 step/s\n",
      "eval loss: 0.09788, accu: 0.97576\n",
      "global step 3610, epoch: 3, batch: 514, loss: 0.01223, accu: 1.00000, speed: 0.79 step/s\n",
      "global step 3620, epoch: 3, batch: 524, loss: 0.00350, accu: 1.00000, speed: 15.76 step/s\n",
      "global step 3630, epoch: 3, batch: 534, loss: 0.00131, accu: 1.00000, speed: 15.60 step/s\n",
      "global step 3640, epoch: 3, batch: 544, loss: 0.00755, accu: 0.99922, speed: 14.76 step/s\n",
      "global step 3650, epoch: 3, batch: 554, loss: 0.13049, accu: 0.99687, speed: 15.60 step/s\n",
      "global step 3660, epoch: 3, batch: 564, loss: 0.00825, accu: 0.99531, speed: 15.28 step/s\n",
      "global step 3670, epoch: 3, batch: 574, loss: 0.26033, accu: 0.99554, speed: 15.83 step/s\n",
      "global step 3680, epoch: 3, batch: 584, loss: 0.00240, accu: 0.99609, speed: 15.46 step/s\n",
      "global step 3690, epoch: 3, batch: 594, loss: 0.08021, accu: 0.99514, speed: 15.78 step/s\n",
      "eval loss: 0.09107, accu: 0.97592\n",
      "global step 3700, epoch: 3, batch: 604, loss: 0.00979, accu: 0.99062, speed: 0.82 step/s\n",
      "global step 3710, epoch: 3, batch: 614, loss: 0.00254, accu: 0.98750, speed: 15.49 step/s\n",
      "global step 3720, epoch: 3, batch: 624, loss: 0.00256, accu: 0.98854, speed: 15.18 step/s\n",
      "global step 3730, epoch: 3, batch: 634, loss: 0.00099, accu: 0.99062, speed: 15.04 step/s\n",
      "global step 3740, epoch: 3, batch: 644, loss: 0.00047, accu: 0.99187, speed: 15.64 step/s\n",
      "global step 3750, epoch: 3, batch: 654, loss: 0.17944, accu: 0.99167, speed: 15.26 step/s\n",
      "global step 3760, epoch: 3, batch: 664, loss: 0.00302, accu: 0.99062, speed: 14.79 step/s\n",
      "global step 3770, epoch: 3, batch: 674, loss: 0.09404, accu: 0.99062, speed: 15.13 step/s\n",
      "global step 3780, epoch: 3, batch: 684, loss: 0.00082, accu: 0.99062, speed: 15.82 step/s\n",
      "eval loss: 0.08290, accu: 0.97867\n",
      "global step 3790, epoch: 3, batch: 694, loss: 0.00138, accu: 0.99375, speed: 0.80 step/s\n",
      "global step 3800, epoch: 3, batch: 704, loss: 0.11067, accu: 0.99219, speed: 15.79 step/s\n",
      "global step 3810, epoch: 3, batch: 714, loss: 0.05870, accu: 0.99167, speed: 16.10 step/s\n",
      "global step 3820, epoch: 3, batch: 724, loss: 0.14777, accu: 0.99141, speed: 15.80 step/s\n",
      "global step 3830, epoch: 3, batch: 734, loss: 0.00116, accu: 0.99062, speed: 15.86 step/s\n",
      "global step 3840, epoch: 3, batch: 744, loss: 0.07127, accu: 0.99062, speed: 15.49 step/s\n",
      "global step 3850, epoch: 3, batch: 754, loss: 0.00500, accu: 0.99107, speed: 13.24 step/s\n",
      "global step 3860, epoch: 3, batch: 764, loss: 0.00211, accu: 0.99180, speed: 15.51 step/s\n",
      "global step 3870, epoch: 3, batch: 774, loss: 0.00114, accu: 0.99201, speed: 15.55 step/s\n",
      "eval loss: 0.08604, accu: 0.97835\n",
      "global step 3880, epoch: 3, batch: 784, loss: 0.00170, accu: 0.99062, speed: 0.81 step/s\n",
      "global step 3890, epoch: 3, batch: 794, loss: 0.00497, accu: 0.99062, speed: 15.74 step/s\n",
      "global step 3900, epoch: 3, batch: 804, loss: 0.01340, accu: 0.99167, speed: 15.74 step/s\n",
      "global step 3910, epoch: 3, batch: 814, loss: 0.00761, accu: 0.99141, speed: 15.20 step/s\n",
      "global step 3920, epoch: 3, batch: 824, loss: 0.00290, accu: 0.99187, speed: 15.41 step/s\n",
      "global step 3930, epoch: 3, batch: 834, loss: 0.00239, accu: 0.99271, speed: 15.48 step/s\n",
      "global step 3940, epoch: 3, batch: 844, loss: 0.00116, accu: 0.99330, speed: 15.37 step/s\n",
      "global step 3950, epoch: 3, batch: 854, loss: 0.00876, accu: 0.99258, speed: 15.28 step/s\n",
      "global step 3960, epoch: 3, batch: 864, loss: 0.05970, accu: 0.99236, speed: 15.57 step/s\n",
      "eval loss: 0.08578, accu: 0.97188\n",
      "global step 3970, epoch: 3, batch: 874, loss: 0.01548, accu: 1.00000, speed: 0.81 step/s\n",
      "global step 3980, epoch: 3, batch: 884, loss: 0.00043, accu: 0.99844, speed: 15.94 step/s\n",
      "global step 3990, epoch: 3, batch: 894, loss: 0.00706, accu: 0.99479, speed: 15.26 step/s\n",
      "global step 4000, epoch: 3, batch: 904, loss: 0.01235, accu: 0.99531, speed: 15.91 step/s\n",
      "global step 4010, epoch: 3, batch: 914, loss: 0.00040, accu: 0.99438, speed: 15.92 step/s\n",
      "global step 4020, epoch: 3, batch: 924, loss: 0.00212, accu: 0.99271, speed: 15.68 step/s\n",
      "global step 4030, epoch: 3, batch: 934, loss: 0.00155, accu: 0.99286, speed: 14.46 step/s\n",
      "global step 4040, epoch: 3, batch: 944, loss: 0.00107, accu: 0.99297, speed: 15.91 step/s\n",
      "global step 4050, epoch: 3, batch: 954, loss: 0.00063, accu: 0.99236, speed: 15.65 step/s\n",
      "eval loss: 0.10315, accu: 0.97511\n",
      "global step 4060, epoch: 3, batch: 964, loss: 0.00566, accu: 0.99375, speed: 0.78 step/s\n",
      "global step 4070, epoch: 3, batch: 974, loss: 0.24426, accu: 0.99062, speed: 15.67 step/s\n",
      "global step 4080, epoch: 3, batch: 984, loss: 0.00694, accu: 0.99271, speed: 15.32 step/s\n",
      "global step 4090, epoch: 3, batch: 994, loss: 0.00372, accu: 0.99375, speed: 15.51 step/s\n",
      "global step 4100, epoch: 3, batch: 1004, loss: 0.01096, accu: 0.99500, speed: 15.99 step/s\n",
      "global step 4110, epoch: 3, batch: 1014, loss: 0.02749, accu: 0.99427, speed: 15.99 step/s\n",
      "global step 4120, epoch: 3, batch: 1024, loss: 0.01259, accu: 0.99420, speed: 14.87 step/s\n",
      "global step 4130, epoch: 3, batch: 1034, loss: 0.01417, accu: 0.99375, speed: 15.69 step/s\n",
      "global step 4140, epoch: 3, batch: 1044, loss: 0.09139, accu: 0.99340, speed: 15.99 step/s\n",
      "eval loss: 0.08952, accu: 0.97867\n",
      "global step 4150, epoch: 3, batch: 1054, loss: 0.00771, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 4160, epoch: 3, batch: 1064, loss: 0.00146, accu: 1.00000, speed: 15.68 step/s\n",
      "global step 4170, epoch: 3, batch: 1074, loss: 0.01802, accu: 0.99375, speed: 15.90 step/s\n",
      "global step 4180, epoch: 3, batch: 1084, loss: 0.00213, accu: 0.99453, speed: 14.75 step/s\n",
      "global step 4190, epoch: 3, batch: 1094, loss: 0.00081, accu: 0.99438, speed: 14.02 step/s\n",
      "global step 4200, epoch: 3, batch: 1104, loss: 0.06763, accu: 0.99427, speed: 14.48 step/s\n",
      "global step 4210, epoch: 3, batch: 1114, loss: 0.02336, accu: 0.99375, speed: 15.57 step/s\n",
      "global step 4220, epoch: 3, batch: 1124, loss: 0.00111, accu: 0.99453, speed: 16.13 step/s\n",
      "global step 4230, epoch: 3, batch: 1134, loss: 0.05619, accu: 0.99375, speed: 15.60 step/s\n",
      "eval loss: 0.09218, accu: 0.98012\n",
      "global step 4240, epoch: 3, batch: 1144, loss: 0.00106, accu: 0.99375, speed: 0.79 step/s\n",
      "global step 4250, epoch: 3, batch: 1154, loss: 0.10564, accu: 0.99531, speed: 15.63 step/s\n",
      "global step 4260, epoch: 3, batch: 1164, loss: 0.00359, accu: 0.99687, speed: 15.31 step/s\n",
      "global step 4270, epoch: 3, batch: 1174, loss: 0.00097, accu: 0.99687, speed: 15.80 step/s\n",
      "global step 4280, epoch: 3, batch: 1184, loss: 0.00042, accu: 0.99562, speed: 16.19 step/s\n",
      "global step 4290, epoch: 3, batch: 1194, loss: 0.00039, accu: 0.99583, speed: 15.65 step/s\n",
      "global step 4300, epoch: 3, batch: 1204, loss: 0.00211, accu: 0.99643, speed: 16.03 step/s\n",
      "global step 4310, epoch: 3, batch: 1214, loss: 0.00092, accu: 0.99648, speed: 16.10 step/s\n",
      "global step 4320, epoch: 3, batch: 1224, loss: 0.00564, accu: 0.99618, speed: 15.76 step/s\n",
      "eval loss: 0.09717, accu: 0.97867\n",
      "global step 4330, epoch: 3, batch: 1234, loss: 0.00046, accu: 0.98438, speed: 0.79 step/s\n",
      "global step 4340, epoch: 3, batch: 1244, loss: 0.00314, accu: 0.98906, speed: 16.01 step/s\n",
      "global step 4350, epoch: 3, batch: 1254, loss: 0.09210, accu: 0.98958, speed: 15.57 step/s\n",
      "global step 4360, epoch: 3, batch: 1264, loss: 0.11074, accu: 0.99062, speed: 15.76 step/s\n",
      "global step 4370, epoch: 3, batch: 1274, loss: 0.00832, accu: 0.99000, speed: 15.78 step/s\n",
      "global step 4380, epoch: 3, batch: 1284, loss: 0.02384, accu: 0.99115, speed: 14.91 step/s\n",
      "global step 4390, epoch: 3, batch: 1294, loss: 0.01079, accu: 0.99107, speed: 15.50 step/s\n",
      "global step 4400, epoch: 3, batch: 1304, loss: 0.00104, accu: 0.99062, speed: 15.73 step/s\n",
      "global step 4410, epoch: 3, batch: 1314, loss: 0.00715, accu: 0.99132, speed: 15.74 step/s\n",
      "eval loss: 0.08013, accu: 0.97883\n",
      "global step 4420, epoch: 3, batch: 1324, loss: 0.00222, accu: 0.99375, speed: 0.82 step/s\n",
      "global step 4430, epoch: 3, batch: 1334, loss: 0.07341, accu: 0.99375, speed: 15.86 step/s\n",
      "global step 4440, epoch: 3, batch: 1344, loss: 0.00446, accu: 0.99375, speed: 15.55 step/s\n",
      "global step 4450, epoch: 3, batch: 1354, loss: 0.00191, accu: 0.99219, speed: 15.74 step/s\n",
      "global step 4460, epoch: 3, batch: 1364, loss: 0.00302, accu: 0.99375, speed: 15.92 step/s\n",
      "global step 4470, epoch: 3, batch: 1374, loss: 0.01281, accu: 0.99427, speed: 15.66 step/s\n",
      "global step 4480, epoch: 3, batch: 1384, loss: 0.00129, accu: 0.99420, speed: 15.19 step/s\n",
      "global step 4490, epoch: 3, batch: 1394, loss: 0.00138, accu: 0.99297, speed: 15.64 step/s\n",
      "global step 4500, epoch: 3, batch: 1404, loss: 0.00242, accu: 0.99306, speed: 15.28 step/s\n",
      "eval loss: 0.08954, accu: 0.97948\n",
      "global step 4510, epoch: 3, batch: 1414, loss: 0.00063, accu: 0.99375, speed: 0.81 step/s\n",
      "global step 4520, epoch: 3, batch: 1424, loss: 0.00362, accu: 0.99375, speed: 15.59 step/s\n",
      "global step 4530, epoch: 3, batch: 1434, loss: 0.01701, accu: 0.99479, speed: 15.64 step/s\n",
      "global step 4540, epoch: 3, batch: 1444, loss: 0.03163, accu: 0.99375, speed: 15.18 step/s\n",
      "global step 4550, epoch: 3, batch: 1454, loss: 0.00273, accu: 0.99375, speed: 15.64 step/s\n",
      "global step 4560, epoch: 3, batch: 1464, loss: 0.00138, accu: 0.99427, speed: 15.93 step/s\n",
      "global step 4570, epoch: 3, batch: 1474, loss: 0.00145, accu: 0.99330, speed: 15.85 step/s\n",
      "global step 4580, epoch: 3, batch: 1484, loss: 0.00255, accu: 0.99258, speed: 15.48 step/s\n",
      "global step 4590, epoch: 3, batch: 1494, loss: 0.00231, accu: 0.99271, speed: 15.70 step/s\n",
      "eval loss: 0.07794, accu: 0.97948\n",
      "global step 4600, epoch: 3, batch: 1504, loss: 0.06262, accu: 0.99375, speed: 0.78 step/s\n",
      "global step 4610, epoch: 3, batch: 1514, loss: 0.00356, accu: 0.99375, speed: 16.26 step/s\n",
      "global step 4620, epoch: 3, batch: 1524, loss: 0.00422, accu: 0.99479, speed: 15.18 step/s\n",
      "global step 4630, epoch: 3, batch: 1534, loss: 0.00489, accu: 0.99297, speed: 15.17 step/s\n",
      "global step 4640, epoch: 3, batch: 1544, loss: 0.00264, accu: 0.99313, speed: 16.40 step/s\n",
      "global step 4650, epoch: 4, batch: 6, loss: 0.00433, accu: 0.99419, speed: 15.46 step/s\n",
      "global step 4660, epoch: 4, batch: 16, loss: 0.01102, accu: 0.99503, speed: 15.94 step/s\n",
      "global step 4670, epoch: 4, batch: 26, loss: 0.00502, accu: 0.99487, speed: 14.20 step/s\n",
      "global step 4680, epoch: 4, batch: 36, loss: 0.00536, accu: 0.99509, speed: 14.44 step/s\n",
      "eval loss: 0.08261, accu: 0.97948\n",
      "global step 4690, epoch: 4, batch: 46, loss: 0.00264, accu: 0.99062, speed: 0.81 step/s\n",
      "global step 4700, epoch: 4, batch: 56, loss: 0.11571, accu: 0.99219, speed: 14.99 step/s\n",
      "global step 4710, epoch: 4, batch: 66, loss: 0.00404, accu: 0.99479, speed: 15.54 step/s\n",
      "global step 4720, epoch: 4, batch: 76, loss: 0.00213, accu: 0.99453, speed: 15.51 step/s\n",
      "global step 4730, epoch: 4, batch: 86, loss: 0.00616, accu: 0.99438, speed: 15.53 step/s\n",
      "global step 4740, epoch: 4, batch: 96, loss: 0.00061, accu: 0.99531, speed: 15.66 step/s\n",
      "global step 4750, epoch: 4, batch: 106, loss: 0.00222, accu: 0.99509, speed: 15.52 step/s\n",
      "global step 4760, epoch: 4, batch: 116, loss: 0.00344, accu: 0.99570, speed: 15.82 step/s\n",
      "global step 4770, epoch: 4, batch: 126, loss: 0.00158, accu: 0.99618, speed: 15.61 step/s\n",
      "eval loss: 0.08493, accu: 0.97948\n",
      "global step 4780, epoch: 4, batch: 136, loss: 0.00412, accu: 1.00000, speed: 0.78 step/s\n",
      "global step 4790, epoch: 4, batch: 146, loss: 0.01412, accu: 0.99844, speed: 15.94 step/s\n",
      "global step 4800, epoch: 4, batch: 156, loss: 0.00104, accu: 0.99896, speed: 15.88 step/s\n",
      "global step 4810, epoch: 4, batch: 166, loss: 0.00140, accu: 0.99844, speed: 16.41 step/s\n",
      "global step 4820, epoch: 4, batch: 176, loss: 0.00426, accu: 0.99813, speed: 15.95 step/s\n",
      "global step 4830, epoch: 4, batch: 186, loss: 0.00063, accu: 0.99792, speed: 15.78 step/s\n",
      "global step 4840, epoch: 4, batch: 196, loss: 0.00044, accu: 0.99821, speed: 15.68 step/s\n",
      "global step 4850, epoch: 4, batch: 206, loss: 0.12015, accu: 0.99766, speed: 15.50 step/s\n",
      "global step 4860, epoch: 4, batch: 216, loss: 0.00374, accu: 0.99757, speed: 15.94 step/s\n",
      "eval loss: 0.09927, accu: 0.97835\n",
      "global step 4870, epoch: 4, batch: 226, loss: 0.00334, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 4880, epoch: 4, batch: 236, loss: 0.00079, accu: 1.00000, speed: 15.15 step/s\n",
      "global step 4890, epoch: 4, batch: 246, loss: 0.00045, accu: 1.00000, speed: 15.23 step/s\n",
      "global step 4900, epoch: 4, batch: 256, loss: 0.00076, accu: 0.99922, speed: 15.55 step/s\n",
      "global step 4910, epoch: 4, batch: 266, loss: 0.00112, accu: 0.99875, speed: 15.14 step/s\n",
      "global step 4920, epoch: 4, batch: 276, loss: 0.00336, accu: 0.99844, speed: 15.93 step/s\n",
      "global step 4930, epoch: 4, batch: 286, loss: 0.00048, accu: 0.99821, speed: 15.98 step/s\n",
      "global step 4940, epoch: 4, batch: 296, loss: 0.00045, accu: 0.99766, speed: 15.84 step/s\n",
      "global step 4950, epoch: 4, batch: 306, loss: 0.10387, accu: 0.99687, speed: 15.19 step/s\n",
      "eval loss: 0.08906, accu: 0.98028\n",
      "global step 4960, epoch: 4, batch: 316, loss: 0.00772, accu: 0.99687, speed: 0.82 step/s\n",
      "global step 4970, epoch: 4, batch: 326, loss: 0.09996, accu: 0.99531, speed: 15.57 step/s\n",
      "global step 4980, epoch: 4, batch: 336, loss: 0.00683, accu: 0.99479, speed: 15.70 step/s\n",
      "global step 4990, epoch: 4, batch: 346, loss: 0.00046, accu: 0.99609, speed: 15.92 step/s\n",
      "global step 5000, epoch: 4, batch: 356, loss: 0.01899, accu: 0.99687, speed: 15.48 step/s\n",
      "global step 5010, epoch: 4, batch: 366, loss: 0.00302, accu: 0.99740, speed: 16.14 step/s\n",
      "global step 5020, epoch: 4, batch: 376, loss: 0.00115, accu: 0.99732, speed: 15.51 step/s\n",
      "global step 5030, epoch: 4, batch: 386, loss: 0.00117, accu: 0.99766, speed: 15.17 step/s\n",
      "global step 5040, epoch: 4, batch: 396, loss: 0.00093, accu: 0.99757, speed: 15.53 step/s\n",
      "eval loss: 0.10010, accu: 0.97899\n",
      "global step 5050, epoch: 4, batch: 406, loss: 0.00044, accu: 0.99687, speed: 0.82 step/s\n",
      "global step 5060, epoch: 4, batch: 416, loss: 0.00923, accu: 0.99531, speed: 16.07 step/s\n",
      "global step 5070, epoch: 4, batch: 426, loss: 0.02224, accu: 0.99479, speed: 15.21 step/s\n",
      "global step 5080, epoch: 4, batch: 436, loss: 0.00145, accu: 0.99609, speed: 15.77 step/s\n",
      "global step 5090, epoch: 4, batch: 446, loss: 0.06242, accu: 0.99562, speed: 14.96 step/s\n",
      "global step 5100, epoch: 4, batch: 456, loss: 0.00053, accu: 0.99583, speed: 14.27 step/s\n",
      "global step 5110, epoch: 4, batch: 466, loss: 0.00065, accu: 0.99598, speed: 14.44 step/s\n",
      "global step 5120, epoch: 4, batch: 476, loss: 0.03359, accu: 0.99570, speed: 16.22 step/s\n",
      "global step 5130, epoch: 4, batch: 486, loss: 0.00034, accu: 0.99618, speed: 14.23 step/s\n",
      "eval loss: 0.09598, accu: 0.98028\n",
      "global step 5140, epoch: 4, batch: 496, loss: 0.00044, accu: 0.99375, speed: 0.80 step/s\n",
      "global step 5150, epoch: 4, batch: 506, loss: 0.00949, accu: 0.99531, speed: 16.36 step/s\n",
      "global step 5160, epoch: 4, batch: 516, loss: 0.14542, accu: 0.99479, speed: 15.71 step/s\n",
      "global step 5170, epoch: 4, batch: 526, loss: 0.00110, accu: 0.99609, speed: 13.86 step/s\n",
      "global step 5180, epoch: 4, batch: 536, loss: 0.00047, accu: 0.99562, speed: 15.59 step/s\n",
      "global step 5190, epoch: 4, batch: 546, loss: 0.00046, accu: 0.99635, speed: 15.38 step/s\n",
      "global step 5200, epoch: 4, batch: 556, loss: 0.06157, accu: 0.99643, speed: 8.19 step/s\n",
      "global step 5210, epoch: 4, batch: 566, loss: 0.00359, accu: 0.99687, speed: 13.25 step/s\n",
      "global step 5220, epoch: 4, batch: 576, loss: 0.00150, accu: 0.99687, speed: 15.34 step/s\n",
      "eval loss: 0.09404, accu: 0.98028\n",
      "global step 5230, epoch: 4, batch: 586, loss: 0.00430, accu: 0.99687, speed: 0.81 step/s\n",
      "global step 5240, epoch: 4, batch: 596, loss: 0.00085, accu: 0.99687, speed: 15.47 step/s\n",
      "global step 5250, epoch: 4, batch: 606, loss: 0.01778, accu: 0.99792, speed: 15.50 step/s\n",
      "global step 5260, epoch: 4, batch: 616, loss: 0.00115, accu: 0.99766, speed: 15.72 step/s\n",
      "global step 5270, epoch: 4, batch: 626, loss: 0.00041, accu: 0.99750, speed: 15.65 step/s\n",
      "global step 5280, epoch: 4, batch: 636, loss: 0.00191, accu: 0.99792, speed: 15.40 step/s\n",
      "global step 5290, epoch: 4, batch: 646, loss: 0.00028, accu: 0.99821, speed: 14.98 step/s\n",
      "global step 5300, epoch: 4, batch: 656, loss: 0.00036, accu: 0.99805, speed: 15.32 step/s\n",
      "global step 5310, epoch: 4, batch: 666, loss: 0.00091, accu: 0.99757, speed: 15.42 step/s\n",
      "eval loss: 0.09842, accu: 0.97948\n",
      "global step 5320, epoch: 4, batch: 676, loss: 0.07297, accu: 0.99375, speed: 0.82 step/s\n",
      "global step 5330, epoch: 4, batch: 686, loss: 0.00017, accu: 0.99687, speed: 16.33 step/s\n",
      "global step 5340, epoch: 4, batch: 696, loss: 0.06630, accu: 0.99479, speed: 15.92 step/s\n",
      "global step 5350, epoch: 4, batch: 706, loss: 0.00102, accu: 0.99609, speed: 15.94 step/s\n",
      "global step 5360, epoch: 4, batch: 716, loss: 0.00134, accu: 0.99687, speed: 15.54 step/s\n",
      "global step 5370, epoch: 4, batch: 726, loss: 0.00118, accu: 0.99635, speed: 15.58 step/s\n",
      "global step 5380, epoch: 4, batch: 736, loss: 0.03659, accu: 0.99643, speed: 15.65 step/s\n",
      "global step 5390, epoch: 4, batch: 746, loss: 0.00019, accu: 0.99687, speed: 15.89 step/s\n",
      "global step 5400, epoch: 4, batch: 756, loss: 0.00082, accu: 0.99722, speed: 15.63 step/s\n",
      "eval loss: 0.10093, accu: 0.97899\n",
      "global step 5410, epoch: 4, batch: 766, loss: 0.00043, accu: 0.99687, speed: 0.83 step/s\n",
      "global step 5420, epoch: 4, batch: 776, loss: 0.00110, accu: 0.99844, speed: 15.78 step/s\n",
      "global step 5430, epoch: 4, batch: 786, loss: 0.00299, accu: 0.99479, speed: 16.13 step/s\n",
      "global step 5440, epoch: 4, batch: 796, loss: 0.00158, accu: 0.99609, speed: 14.41 step/s\n",
      "global step 5450, epoch: 4, batch: 806, loss: 0.00328, accu: 0.99562, speed: 13.13 step/s\n",
      "global step 5460, epoch: 4, batch: 816, loss: 0.21310, accu: 0.99531, speed: 15.75 step/s\n",
      "global step 5470, epoch: 4, batch: 826, loss: 0.00036, accu: 0.99554, speed: 16.11 step/s\n",
      "global step 5480, epoch: 4, batch: 836, loss: 0.00911, accu: 0.99609, speed: 15.23 step/s\n",
      "global step 5490, epoch: 4, batch: 846, loss: 0.00033, accu: 0.99583, speed: 15.24 step/s\n",
      "eval loss: 0.10219, accu: 0.97964\n",
      "global step 5500, epoch: 4, batch: 856, loss: 0.00235, accu: 1.00000, speed: 0.82 step/s\n",
      "global step 5510, epoch: 4, batch: 866, loss: 0.02905, accu: 0.99531, speed: 16.08 step/s\n",
      "global step 5520, epoch: 4, batch: 876, loss: 0.00037, accu: 0.99479, speed: 14.76 step/s\n",
      "global step 5530, epoch: 4, batch: 886, loss: 0.00047, accu: 0.99453, speed: 14.12 step/s\n",
      "global step 5540, epoch: 4, batch: 896, loss: 0.00165, accu: 0.99562, speed: 14.65 step/s\n",
      "global step 5550, epoch: 4, batch: 906, loss: 0.00034, accu: 0.99427, speed: 14.23 step/s\n",
      "global step 5560, epoch: 4, batch: 916, loss: 0.00039, accu: 0.99509, speed: 14.11 step/s\n",
      "global step 5570, epoch: 4, batch: 926, loss: 0.00035, accu: 0.99414, speed: 15.42 step/s\n",
      "global step 5580, epoch: 4, batch: 936, loss: 0.00383, accu: 0.99410, speed: 15.67 step/s\n",
      "eval loss: 0.08528, accu: 0.98174\n",
      "global step 5590, epoch: 4, batch: 946, loss: 0.02775, accu: 0.99375, speed: 0.77 step/s\n",
      "global step 5600, epoch: 4, batch: 956, loss: 0.00196, accu: 0.99687, speed: 15.59 step/s\n",
      "global step 5610, epoch: 4, batch: 966, loss: 0.01266, accu: 0.99687, speed: 15.77 step/s\n",
      "global step 5620, epoch: 4, batch: 976, loss: 0.00185, accu: 0.99687, speed: 15.83 step/s\n",
      "global step 5630, epoch: 4, batch: 986, loss: 0.00062, accu: 0.99750, speed: 15.33 step/s\n",
      "global step 5640, epoch: 4, batch: 996, loss: 0.00085, accu: 0.99792, speed: 15.67 step/s\n",
      "global step 5650, epoch: 4, batch: 1006, loss: 0.00218, accu: 0.99821, speed: 15.46 step/s\n",
      "global step 5660, epoch: 4, batch: 1016, loss: 0.00067, accu: 0.99766, speed: 15.37 step/s\n",
      "global step 5670, epoch: 4, batch: 1026, loss: 0.01351, accu: 0.99722, speed: 15.66 step/s\n",
      "eval loss: 0.08954, accu: 0.98174\n",
      "global step 5680, epoch: 4, batch: 1036, loss: 0.00083, accu: 0.99375, speed: 0.77 step/s\n",
      "global step 5690, epoch: 4, batch: 1046, loss: 0.02672, accu: 0.99375, speed: 14.00 step/s\n",
      "global step 5700, epoch: 4, batch: 1056, loss: 0.00359, accu: 0.99479, speed: 15.20 step/s\n",
      "global step 5710, epoch: 4, batch: 1066, loss: 0.01241, accu: 0.99531, speed: 15.90 step/s\n",
      "global step 5720, epoch: 4, batch: 1076, loss: 0.00076, accu: 0.99625, speed: 15.35 step/s\n",
      "global step 5730, epoch: 4, batch: 1086, loss: 0.00113, accu: 0.99635, speed: 15.72 step/s\n",
      "global step 5740, epoch: 4, batch: 1096, loss: 0.00032, accu: 0.99687, speed: 15.52 step/s\n",
      "global step 5750, epoch: 4, batch: 1106, loss: 0.00166, accu: 0.99687, speed: 15.60 step/s\n",
      "global step 5760, epoch: 4, batch: 1116, loss: 0.00165, accu: 0.99722, speed: 15.09 step/s\n",
      "eval loss: 0.08707, accu: 0.98109\n",
      "global step 5770, epoch: 4, batch: 1126, loss: 0.00067, accu: 0.99687, speed: 0.81 step/s\n",
      "global step 5780, epoch: 4, batch: 1136, loss: 0.00053, accu: 0.99844, speed: 14.40 step/s\n",
      "global step 5790, epoch: 4, batch: 1146, loss: 0.00114, accu: 0.99687, speed: 15.22 step/s\n",
      "global step 5800, epoch: 4, batch: 1156, loss: 0.00037, accu: 0.99766, speed: 14.88 step/s\n",
      "global step 5810, epoch: 4, batch: 1166, loss: 0.00531, accu: 0.99813, speed: 15.22 step/s\n",
      "global step 5820, epoch: 4, batch: 1176, loss: 0.00110, accu: 0.99844, speed: 15.58 step/s\n",
      "global step 5830, epoch: 4, batch: 1186, loss: 0.00602, accu: 0.99866, speed: 15.67 step/s\n",
      "global step 5840, epoch: 4, batch: 1196, loss: 0.00066, accu: 0.99844, speed: 16.13 step/s\n",
      "global step 5850, epoch: 4, batch: 1206, loss: 0.00827, accu: 0.99861, speed: 15.97 step/s\n",
      "eval loss: 0.08988, accu: 0.98174\n",
      "global step 5860, epoch: 4, batch: 1216, loss: 0.00082, accu: 0.99687, speed: 0.80 step/s\n",
      "global step 5870, epoch: 4, batch: 1226, loss: 0.00173, accu: 0.99687, speed: 15.78 step/s\n",
      "global step 5880, epoch: 4, batch: 1236, loss: 0.00031, accu: 0.99583, speed: 15.34 step/s\n",
      "global step 5890, epoch: 4, batch: 1246, loss: 0.00892, accu: 0.99609, speed: 15.90 step/s\n",
      "global step 5900, epoch: 4, batch: 1256, loss: 0.00023, accu: 0.99625, speed: 15.60 step/s\n",
      "global step 5910, epoch: 4, batch: 1266, loss: 0.11310, accu: 0.99583, speed: 15.13 step/s\n",
      "global step 5920, epoch: 4, batch: 1276, loss: 0.00126, accu: 0.99598, speed: 15.70 step/s\n",
      "global step 5930, epoch: 4, batch: 1286, loss: 0.00065, accu: 0.99570, speed: 15.70 step/s\n",
      "global step 5940, epoch: 4, batch: 1296, loss: 0.00068, accu: 0.99549, speed: 15.64 step/s\n",
      "eval loss: 0.09275, accu: 0.98093\n",
      "global step 5950, epoch: 4, batch: 1306, loss: 0.00383, accu: 1.00000, speed: 0.82 step/s\n",
      "global step 5960, epoch: 4, batch: 1316, loss: 0.00178, accu: 1.00000, speed: 14.74 step/s\n",
      "global step 5970, epoch: 4, batch: 1326, loss: 0.00060, accu: 0.99896, speed: 14.23 step/s\n",
      "global step 5980, epoch: 4, batch: 1336, loss: 0.00060, accu: 0.99687, speed: 16.09 step/s\n",
      "global step 5990, epoch: 4, batch: 1346, loss: 0.00122, accu: 0.99562, speed: 15.43 step/s\n",
      "global step 6000, epoch: 4, batch: 1356, loss: 0.00300, accu: 0.99635, speed: 15.52 step/s\n",
      "global step 6010, epoch: 4, batch: 1366, loss: 0.00068, accu: 0.99687, speed: 15.50 step/s\n",
      "global step 6020, epoch: 4, batch: 1376, loss: 0.00960, accu: 0.99727, speed: 15.48 step/s\n",
      "global step 6030, epoch: 4, batch: 1386, loss: 0.00072, accu: 0.99757, speed: 15.63 step/s\n",
      "eval loss: 0.08871, accu: 0.98109\n",
      "global step 6040, epoch: 4, batch: 1396, loss: 0.09416, accu: 0.99687, speed: 0.81 step/s\n",
      "global step 6050, epoch: 4, batch: 1406, loss: 0.00081, accu: 0.99687, speed: 15.13 step/s\n",
      "global step 6060, epoch: 4, batch: 1416, loss: 0.00099, accu: 0.99687, speed: 15.70 step/s\n",
      "global step 6070, epoch: 4, batch: 1426, loss: 0.00176, accu: 0.99687, speed: 15.61 step/s\n",
      "global step 6080, epoch: 4, batch: 1436, loss: 0.00072, accu: 0.99687, speed: 15.51 step/s\n",
      "global step 6090, epoch: 4, batch: 1446, loss: 0.01538, accu: 0.99635, speed: 15.22 step/s\n",
      "global step 6100, epoch: 4, batch: 1456, loss: 0.00079, accu: 0.99643, speed: 15.29 step/s\n",
      "global step 6110, epoch: 4, batch: 1466, loss: 0.00046, accu: 0.99648, speed: 15.71 step/s\n",
      "global step 6120, epoch: 4, batch: 1476, loss: 0.02971, accu: 0.99618, speed: 15.63 step/s\n",
      "eval loss: 0.09342, accu: 0.98045\n",
      "global step 6130, epoch: 4, batch: 1486, loss: 0.00037, accu: 0.98750, speed: 0.79 step/s\n",
      "global step 6140, epoch: 4, batch: 1496, loss: 0.00088, accu: 0.99375, speed: 15.68 step/s\n",
      "global step 6150, epoch: 4, batch: 1506, loss: 0.00574, accu: 0.99479, speed: 15.62 step/s\n",
      "global step 6160, epoch: 4, batch: 1516, loss: 0.00189, accu: 0.99531, speed: 15.69 step/s\n",
      "global step 6170, epoch: 4, batch: 1526, loss: 0.00131, accu: 0.99438, speed: 16.01 step/s\n",
      "global step 6180, epoch: 4, batch: 1536, loss: 0.00104, accu: 0.99479, speed: 15.98 step/s\n",
      "global step 6190, epoch: 4, batch: 1546, loss: 0.00217, accu: 0.99509, speed: 18.37 step/s\n",
      "global step 6200, epoch: 5, batch: 8, loss: 0.00112, accu: 0.99487, speed: 13.81 step/s\n",
      "global step 6210, epoch: 5, batch: 18, loss: 0.00064, accu: 0.99544, speed: 16.06 step/s\n",
      "eval loss: 0.09479, accu: 0.97867\n",
      "global step 6220, epoch: 5, batch: 28, loss: 0.00054, accu: 1.00000, speed: 0.83 step/s\n",
      "global step 6230, epoch: 5, batch: 38, loss: 0.00041, accu: 1.00000, speed: 15.83 step/s\n",
      "global step 6240, epoch: 5, batch: 48, loss: 0.00030, accu: 1.00000, speed: 15.79 step/s\n",
      "global step 6250, epoch: 5, batch: 58, loss: 0.00070, accu: 1.00000, speed: 15.24 step/s\n",
      "global step 6260, epoch: 5, batch: 68, loss: 0.00627, accu: 1.00000, speed: 15.69 step/s\n",
      "global step 6270, epoch: 5, batch: 78, loss: 0.00075, accu: 0.99948, speed: 15.63 step/s\n",
      "global step 6280, epoch: 5, batch: 88, loss: 0.00078, accu: 0.99911, speed: 15.25 step/s\n",
      "global step 6290, epoch: 5, batch: 98, loss: 0.00031, accu: 0.99922, speed: 15.74 step/s\n",
      "global step 6300, epoch: 5, batch: 108, loss: 0.05635, accu: 0.99896, speed: 15.55 step/s\n",
      "eval loss: 0.09492, accu: 0.97996\n",
      "global step 6310, epoch: 5, batch: 118, loss: 0.00056, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 6320, epoch: 5, batch: 128, loss: 0.00128, accu: 1.00000, speed: 15.81 step/s\n",
      "global step 6330, epoch: 5, batch: 138, loss: 0.00141, accu: 1.00000, speed: 15.96 step/s\n",
      "global step 6340, epoch: 5, batch: 148, loss: 0.00448, accu: 0.99922, speed: 15.56 step/s\n",
      "global step 6350, epoch: 5, batch: 158, loss: 0.00178, accu: 0.99938, speed: 15.55 step/s\n",
      "global step 6360, epoch: 5, batch: 168, loss: 0.00106, accu: 0.99948, speed: 15.72 step/s\n",
      "global step 6370, epoch: 5, batch: 178, loss: 0.00662, accu: 0.99911, speed: 15.49 step/s\n",
      "global step 6380, epoch: 5, batch: 188, loss: 0.00177, accu: 0.99922, speed: 15.73 step/s\n",
      "global step 6390, epoch: 5, batch: 198, loss: 0.01332, accu: 0.99931, speed: 15.75 step/s\n",
      "eval loss: 0.09734, accu: 0.98077\n",
      "global step 6400, epoch: 5, batch: 208, loss: 0.00063, accu: 1.00000, speed: 0.75 step/s\n",
      "global step 6410, epoch: 5, batch: 218, loss: 0.00136, accu: 0.99844, speed: 15.07 step/s\n",
      "global step 6420, epoch: 5, batch: 228, loss: 0.00164, accu: 0.99792, speed: 15.21 step/s\n",
      "global step 6430, epoch: 5, batch: 238, loss: 0.00074, accu: 0.99844, speed: 16.05 step/s\n",
      "global step 6440, epoch: 5, batch: 248, loss: 0.00023, accu: 0.99875, speed: 15.44 step/s\n",
      "global step 6450, epoch: 5, batch: 258, loss: 0.00031, accu: 0.99896, speed: 15.47 step/s\n",
      "global step 6460, epoch: 5, batch: 268, loss: 0.00056, accu: 0.99866, speed: 15.74 step/s\n",
      "global step 6470, epoch: 5, batch: 278, loss: 0.00036, accu: 0.99883, speed: 15.30 step/s\n",
      "global step 6480, epoch: 5, batch: 288, loss: 0.00181, accu: 0.99896, speed: 15.86 step/s\n",
      "eval loss: 0.09912, accu: 0.98012\n",
      "global step 6490, epoch: 5, batch: 298, loss: 0.00196, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 6500, epoch: 5, batch: 308, loss: 0.00030, accu: 1.00000, speed: 15.59 step/s\n",
      "global step 6510, epoch: 5, batch: 318, loss: 0.00027, accu: 1.00000, speed: 15.82 step/s\n",
      "global step 6520, epoch: 5, batch: 328, loss: 0.00030, accu: 0.99922, speed: 15.39 step/s\n",
      "global step 6530, epoch: 5, batch: 338, loss: 0.00017, accu: 0.99938, speed: 15.59 step/s\n",
      "global step 6540, epoch: 5, batch: 348, loss: 0.00079, accu: 0.99948, speed: 16.06 step/s\n",
      "global step 6550, epoch: 5, batch: 358, loss: 0.00026, accu: 0.99911, speed: 15.99 step/s\n",
      "global step 6560, epoch: 5, batch: 368, loss: 0.01049, accu: 0.99883, speed: 15.66 step/s\n",
      "global step 6570, epoch: 5, batch: 378, loss: 0.00086, accu: 0.99861, speed: 15.63 step/s\n",
      "eval loss: 0.09406, accu: 0.98077\n",
      "global step 6580, epoch: 5, batch: 388, loss: 0.00054, accu: 0.99375, speed: 0.81 step/s\n",
      "global step 6590, epoch: 5, batch: 398, loss: 0.05720, accu: 0.99531, speed: 15.49 step/s\n",
      "global step 6600, epoch: 5, batch: 408, loss: 0.00288, accu: 0.99479, speed: 15.83 step/s\n",
      "global step 6610, epoch: 5, batch: 418, loss: 0.00032, accu: 0.99609, speed: 15.50 step/s\n",
      "global step 6620, epoch: 5, batch: 428, loss: 0.00029, accu: 0.99687, speed: 15.53 step/s\n",
      "global step 6630, epoch: 5, batch: 438, loss: 0.00139, accu: 0.99740, speed: 15.51 step/s\n",
      "global step 6640, epoch: 5, batch: 448, loss: 0.00861, accu: 0.99777, speed: 16.15 step/s\n",
      "global step 6650, epoch: 5, batch: 458, loss: 0.00420, accu: 0.99766, speed: 15.80 step/s\n",
      "global step 6660, epoch: 5, batch: 468, loss: 0.00034, accu: 0.99792, speed: 16.05 step/s\n",
      "eval loss: 0.09901, accu: 0.97964\n",
      "global step 6670, epoch: 5, batch: 478, loss: 0.00027, accu: 1.00000, speed: 0.77 step/s\n",
      "global step 6680, epoch: 5, batch: 488, loss: 0.11761, accu: 0.99844, speed: 15.42 step/s\n",
      "global step 6690, epoch: 5, batch: 498, loss: 0.00017, accu: 0.99896, speed: 15.77 step/s\n",
      "global step 6700, epoch: 5, batch: 508, loss: 0.00091, accu: 0.99922, speed: 14.26 step/s\n",
      "global step 6710, epoch: 5, batch: 518, loss: 0.00062, accu: 0.99938, speed: 16.83 step/s\n",
      "global step 6720, epoch: 5, batch: 528, loss: 0.00032, accu: 0.99948, speed: 15.25 step/s\n",
      "global step 6730, epoch: 5, batch: 538, loss: 0.00129, accu: 0.99955, speed: 15.07 step/s\n",
      "global step 6740, epoch: 5, batch: 548, loss: 0.00041, accu: 0.99961, speed: 15.47 step/s\n",
      "global step 6750, epoch: 5, batch: 558, loss: 0.00043, accu: 0.99965, speed: 15.77 step/s\n",
      "eval loss: 0.10418, accu: 0.97964\n",
      "global step 6760, epoch: 5, batch: 568, loss: 0.00077, accu: 0.99687, speed: 0.81 step/s\n",
      "global step 6770, epoch: 5, batch: 578, loss: 0.00795, accu: 0.99844, speed: 15.62 step/s\n",
      "global step 6780, epoch: 5, batch: 588, loss: 0.00591, accu: 0.99896, speed: 15.89 step/s\n",
      "global step 6790, epoch: 5, batch: 598, loss: 0.00040, accu: 0.99922, speed: 15.87 step/s\n",
      "global step 6800, epoch: 5, batch: 608, loss: 0.00052, accu: 0.99938, speed: 15.56 step/s\n",
      "global step 6810, epoch: 5, batch: 618, loss: 0.00060, accu: 0.99948, speed: 16.09 step/s\n",
      "global step 6820, epoch: 5, batch: 628, loss: 0.00068, accu: 0.99911, speed: 17.10 step/s\n",
      "global step 6830, epoch: 5, batch: 638, loss: 0.00116, accu: 0.99922, speed: 15.58 step/s\n",
      "global step 6840, epoch: 5, batch: 648, loss: 0.00026, accu: 0.99931, speed: 15.26 step/s\n",
      "eval loss: 0.10177, accu: 0.98028\n",
      "global step 6850, epoch: 5, batch: 658, loss: 0.00110, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 6860, epoch: 5, batch: 668, loss: 0.05928, accu: 0.99844, speed: 15.24 step/s\n",
      "global step 6870, epoch: 5, batch: 678, loss: 0.00021, accu: 0.99896, speed: 15.65 step/s\n",
      "global step 6880, epoch: 5, batch: 688, loss: 0.00024, accu: 0.99922, speed: 15.46 step/s\n",
      "global step 6890, epoch: 5, batch: 698, loss: 0.00033, accu: 0.99938, speed: 15.62 step/s\n",
      "global step 6900, epoch: 5, batch: 708, loss: 0.00048, accu: 0.99948, speed: 15.38 step/s\n",
      "global step 6910, epoch: 5, batch: 718, loss: 0.00076, accu: 0.99911, speed: 14.80 step/s\n",
      "global step 6920, epoch: 5, batch: 728, loss: 0.00050, accu: 0.99844, speed: 15.02 step/s\n",
      "global step 6930, epoch: 5, batch: 738, loss: 0.00561, accu: 0.99861, speed: 14.83 step/s\n",
      "eval loss: 0.09485, accu: 0.98174\n",
      "global step 6940, epoch: 5, batch: 748, loss: 0.00014, accu: 0.99687, speed: 0.78 step/s\n",
      "global step 6950, epoch: 5, batch: 758, loss: 0.09790, accu: 0.99687, speed: 15.39 step/s\n",
      "global step 6960, epoch: 5, batch: 768, loss: 0.00036, accu: 0.99792, speed: 15.47 step/s\n",
      "global step 6970, epoch: 5, batch: 778, loss: 0.00260, accu: 0.99844, speed: 15.47 step/s\n",
      "global step 6980, epoch: 5, batch: 788, loss: 0.00784, accu: 0.99813, speed: 15.03 step/s\n",
      "global step 6990, epoch: 5, batch: 798, loss: 0.00176, accu: 0.99844, speed: 15.79 step/s\n",
      "global step 7000, epoch: 5, batch: 808, loss: 0.00049, accu: 0.99866, speed: 15.71 step/s\n",
      "global step 7010, epoch: 5, batch: 818, loss: 0.00030, accu: 0.99844, speed: 16.07 step/s\n",
      "global step 7020, epoch: 5, batch: 828, loss: 0.00021, accu: 0.99826, speed: 15.38 step/s\n",
      "eval loss: 0.10077, accu: 0.97996\n",
      "global step 7030, epoch: 5, batch: 838, loss: 0.00072, accu: 1.00000, speed: 0.81 step/s\n",
      "global step 7040, epoch: 5, batch: 848, loss: 0.00417, accu: 1.00000, speed: 15.26 step/s\n",
      "global step 7050, epoch: 5, batch: 858, loss: 0.00033, accu: 0.99896, speed: 15.07 step/s\n",
      "global step 7060, epoch: 5, batch: 868, loss: 0.00020, accu: 0.99922, speed: 14.64 step/s\n",
      "global step 7070, epoch: 5, batch: 878, loss: 0.00022, accu: 0.99938, speed: 16.07 step/s\n",
      "global step 7080, epoch: 5, batch: 888, loss: 0.00117, accu: 0.99948, speed: 15.45 step/s\n",
      "global step 7090, epoch: 5, batch: 898, loss: 0.01279, accu: 0.99955, speed: 16.09 step/s\n",
      "global step 7100, epoch: 5, batch: 908, loss: 0.00024, accu: 0.99922, speed: 15.69 step/s\n",
      "global step 7110, epoch: 5, batch: 918, loss: 0.00035, accu: 0.99931, speed: 15.12 step/s\n",
      "eval loss: 0.10147, accu: 0.98028\n",
      "global step 7120, epoch: 5, batch: 928, loss: 0.00014, accu: 1.00000, speed: 0.81 step/s\n",
      "global step 7130, epoch: 5, batch: 938, loss: 0.00030, accu: 1.00000, speed: 16.09 step/s\n",
      "global step 7140, epoch: 5, batch: 948, loss: 0.00016, accu: 1.00000, speed: 15.59 step/s\n",
      "global step 7150, epoch: 5, batch: 958, loss: 0.00048, accu: 1.00000, speed: 15.57 step/s\n",
      "global step 7160, epoch: 5, batch: 968, loss: 0.00011, accu: 1.00000, speed: 15.92 step/s\n",
      "global step 7170, epoch: 5, batch: 978, loss: 0.00055, accu: 1.00000, speed: 15.37 step/s\n",
      "global step 7180, epoch: 5, batch: 988, loss: 0.00023, accu: 0.99955, speed: 15.46 step/s\n",
      "global step 7190, epoch: 5, batch: 998, loss: 0.00095, accu: 0.99961, speed: 15.55 step/s\n",
      "global step 7200, epoch: 5, batch: 1008, loss: 0.00053, accu: 0.99931, speed: 13.45 step/s\n",
      "eval loss: 0.10023, accu: 0.98077\n",
      "global step 7210, epoch: 5, batch: 1018, loss: 0.00014, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 7220, epoch: 5, batch: 1028, loss: 0.00014, accu: 1.00000, speed: 10.15 step/s\n",
      "global step 7230, epoch: 5, batch: 1038, loss: 0.00063, accu: 1.00000, speed: 16.01 step/s\n",
      "global step 7240, epoch: 5, batch: 1048, loss: 0.00046, accu: 0.99922, speed: 15.72 step/s\n",
      "global step 7250, epoch: 5, batch: 1058, loss: 0.00162, accu: 0.99938, speed: 15.21 step/s\n",
      "global step 7260, epoch: 5, batch: 1068, loss: 0.00018, accu: 0.99948, speed: 15.58 step/s\n",
      "global step 7270, epoch: 5, batch: 1078, loss: 0.00012, accu: 0.99955, speed: 15.41 step/s\n",
      "global step 7280, epoch: 5, batch: 1088, loss: 0.00026, accu: 0.99961, speed: 15.46 step/s\n",
      "global step 7290, epoch: 5, batch: 1098, loss: 0.00016, accu: 0.99965, speed: 15.91 step/s\n",
      "eval loss: 0.10067, accu: 0.98125\n",
      "global step 7300, epoch: 5, batch: 1108, loss: 0.20189, accu: 0.99375, speed: 0.80 step/s\n",
      "global step 7310, epoch: 5, batch: 1118, loss: 0.00038, accu: 0.99687, speed: 15.46 step/s\n",
      "global step 7320, epoch: 5, batch: 1128, loss: 0.00121, accu: 0.99792, speed: 15.61 step/s\n",
      "global step 7330, epoch: 5, batch: 1138, loss: 0.00096, accu: 0.99766, speed: 15.43 step/s\n",
      "global step 7340, epoch: 5, batch: 1148, loss: 0.02514, accu: 0.99750, speed: 15.88 step/s\n",
      "global step 7350, epoch: 5, batch: 1158, loss: 0.01558, accu: 0.99740, speed: 15.46 step/s\n",
      "global step 7360, epoch: 5, batch: 1168, loss: 0.00021, accu: 0.99777, speed: 15.64 step/s\n",
      "global step 7370, epoch: 5, batch: 1178, loss: 0.00009, accu: 0.99805, speed: 15.51 step/s\n",
      "global step 7380, epoch: 5, batch: 1188, loss: 0.00012, accu: 0.99792, speed: 15.64 step/s\n",
      "eval loss: 0.10071, accu: 0.98045\n",
      "global step 7390, epoch: 5, batch: 1198, loss: 0.00177, accu: 0.99687, speed: 0.81 step/s\n",
      "global step 7400, epoch: 5, batch: 1208, loss: 0.00009, accu: 0.99844, speed: 14.79 step/s\n",
      "global step 7410, epoch: 5, batch: 1218, loss: 0.00020, accu: 0.99687, speed: 15.49 step/s\n",
      "global step 7420, epoch: 5, batch: 1228, loss: 0.00043, accu: 0.99687, speed: 15.44 step/s\n",
      "global step 7430, epoch: 5, batch: 1238, loss: 0.00023, accu: 0.99750, speed: 15.72 step/s\n",
      "global step 7440, epoch: 5, batch: 1248, loss: 0.02097, accu: 0.99792, speed: 15.34 step/s\n",
      "global step 7450, epoch: 5, batch: 1258, loss: 0.08848, accu: 0.99777, speed: 16.30 step/s\n",
      "global step 7460, epoch: 5, batch: 1268, loss: 0.00056, accu: 0.99766, speed: 15.52 step/s\n",
      "global step 7470, epoch: 5, batch: 1278, loss: 0.15722, accu: 0.99722, speed: 15.60 step/s\n",
      "eval loss: 0.10220, accu: 0.98012\n",
      "global step 7480, epoch: 5, batch: 1288, loss: 0.00502, accu: 1.00000, speed: 0.82 step/s\n",
      "global step 7490, epoch: 5, batch: 1298, loss: 0.00514, accu: 1.00000, speed: 10.58 step/s\n",
      "global step 7500, epoch: 5, batch: 1308, loss: 0.00034, accu: 0.99896, speed: 10.87 step/s\n",
      "global step 7510, epoch: 5, batch: 1318, loss: 0.00116, accu: 0.99922, speed: 15.07 step/s\n",
      "global step 7520, epoch: 5, batch: 1328, loss: 0.00013, accu: 0.99938, speed: 15.55 step/s\n",
      "global step 7530, epoch: 5, batch: 1338, loss: 0.00068, accu: 0.99896, speed: 15.65 step/s\n",
      "global step 7540, epoch: 5, batch: 1348, loss: 0.00849, accu: 0.99821, speed: 15.55 step/s\n",
      "global step 7550, epoch: 5, batch: 1358, loss: 0.00035, accu: 0.99844, speed: 15.37 step/s\n",
      "global step 7560, epoch: 5, batch: 1368, loss: 0.00119, accu: 0.99861, speed: 14.03 step/s\n",
      "eval loss: 0.10124, accu: 0.98028\n",
      "global step 7570, epoch: 5, batch: 1378, loss: 0.00037, accu: 1.00000, speed: 0.80 step/s\n",
      "global step 7580, epoch: 5, batch: 1388, loss: 0.00020, accu: 1.00000, speed: 15.59 step/s\n",
      "global step 7590, epoch: 5, batch: 1398, loss: 0.00038, accu: 1.00000, speed: 15.86 step/s\n",
      "global step 7600, epoch: 5, batch: 1408, loss: 0.00019, accu: 1.00000, speed: 15.29 step/s\n",
      "global step 7610, epoch: 5, batch: 1418, loss: 0.00017, accu: 1.00000, speed: 16.54 step/s\n",
      "global step 7620, epoch: 5, batch: 1428, loss: 0.00207, accu: 1.00000, speed: 15.73 step/s\n",
      "global step 7630, epoch: 5, batch: 1438, loss: 0.00024, accu: 0.99955, speed: 15.85 step/s\n",
      "global step 7640, epoch: 5, batch: 1448, loss: 0.00077, accu: 0.99961, speed: 15.80 step/s\n",
      "global step 7650, epoch: 5, batch: 1458, loss: 0.00364, accu: 0.99931, speed: 15.35 step/s\n",
      "eval loss: 0.10051, accu: 0.98077\n",
      "global step 7660, epoch: 5, batch: 1468, loss: 0.00022, accu: 1.00000, speed: 0.81 step/s\n",
      "global step 7670, epoch: 5, batch: 1478, loss: 0.00077, accu: 1.00000, speed: 15.45 step/s\n",
      "global step 7680, epoch: 5, batch: 1488, loss: 0.03393, accu: 0.99896, speed: 15.13 step/s\n",
      "global step 7690, epoch: 5, batch: 1498, loss: 0.00017, accu: 0.99922, speed: 15.67 step/s\n",
      "global step 7700, epoch: 5, batch: 1508, loss: 0.00064, accu: 0.99875, speed: 15.84 step/s\n",
      "global step 7710, epoch: 5, batch: 1518, loss: 0.00103, accu: 0.99844, speed: 15.93 step/s\n",
      "global step 7720, epoch: 5, batch: 1528, loss: 0.04352, accu: 0.99821, speed: 15.96 step/s\n",
      "global step 7730, epoch: 5, batch: 1538, loss: 0.00036, accu: 0.99844, speed: 15.84 step/s\n",
      "global step 7740, epoch: 5, batch: 1548, loss: 0.00020, accu: 0.99860, speed: 20.05 step/s\n",
      "eval loss: 0.09993, accu: 0.98093\n",
      "test result...\n",
      "eval loss: 0.09451, accu: 0.98158\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "通过使用可视化工具可视化，可以看出效果如下图所示\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/30184e47812e4f088f33908d5877e036d2533d5bdc2945c8b96a73e8f6965793\" width=\"  \"></div>\n",
    "<center>使用roberta模型训练</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting mmpi\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/98/85/41eade42579f75c07afceae7081d6229fadbe427ada12bfee85e5c8c80a0/mmpi-0.1.3-py3-none-any.whl (84 kB)\n",
      "     |████████████████████████████████| 84 kB 3.7 MB/s             \n",
      "\u001b[?25hCollecting olefile==0.46\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112 kB)\n",
      "     |████████████████████████████████| 112 kB 20.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting yara-python\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/52/792913c765ea17bdaf3d91fb2b358a337acf8403079b57329c41f1848213/yara-python-4.1.3.tar.gz (426 kB)\n",
      "     |████████████████████████████████| 426 kB 12.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: olefile, yara-python\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=991edf6b6a70cd0050d951faddc94efb9c666a210f915f65829d346bcfeff981\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/56/e1/3d/2d29e9896c1708415617d30668a86b473846c4dc48817a3e85\n",
      "  Building wheel for yara-python (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yara-python: filename=yara_python-4.1.3-cp37-cp37m-linux_x86_64.whl size=613660 sha256=a3a8daf4c1ee6555851c53ad7512506889e35e371b7f13555b9aff6c82047835\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/22/86/66/73eb51c078dfb9ffecb5c5e9f5c4862885cf8e5de69a84b9e5\n",
      "Successfully built olefile yara-python\n",
      "Installing collected packages: yara-python, olefile, mmpi\n",
      "Successfully installed mmpi-0.1.3 olefile-0.46 yara-python-4.1.3\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mmpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-02-27 16:52:37,473] [    INFO]\u001b[0m - Already cached /home/aistudio/.paddlenlp/models/rbt3/vocab.txt\u001b[0m\n",
      "YaraScanner need yara-python module, please install. pip install yara-python\n",
      "\u001b[32m[2022-02-27 16:52:37,532] [    INFO]\u001b[0m - Already cached /home/aistudio/.paddlenlp/models/rbt3/rbt3_chn_large.pdparams\u001b[0m\n",
      "W0227 16:52:37.533650 23497 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0227 16:52:37.537231 23497 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "Loaded parameters from ./checkpoint/model_360/model_state.pdparams\n",
      "Data: 低点代开发票 \t Lable: 垃圾邮件\n",
      "Data: 深圳协恒实业有限公司 \t Lable: 垃圾邮件\n",
      "Data: 帮帮忙啊 \t Lable: 垃圾邮件\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python predict.py --params_path=./checkpoint/model_360/model_state.pdparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4、总结与升华\n",
    "通过自己动手去实现这个项目后，针对于之前只能通过降低库函数版本以及框架来实现的算法，现在可以做到根据现有最新的框架去实现自己想要的算法，同时本文完成的是关于中文垃圾邮件的分类问题，使用的是bert模型和roberta模型，网上有人评论则提到，这两个模型的不同之处在于后者其实是使用名副其实的暴力调参法，在跑实验的过程中，也可以看出，使用bert模型去训练数据的话，其训练速度比较慢，如下图所示在其性能监控中可以看出\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/ba6d1473cf004d8e8bdf204d8ac29e0fa6f9dfc59a8d44c6b9a872a8ed90cc41\" width=\"  \"></div>\n",
    "<center>使用bert算法性能监控</center>\n",
    "\n",
    "而且所需花费的时间也很长，相反，若使用的是roberta，其算法的收敛速度，训练速度相比于bert来说都有一定的改进，如下图所示\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/dc3dc4c52d3042158b3f328814445fdcaf3b276bf3d540bbb94067eeaaaf0671\" width=\"  \"></div>\n",
    "<center>使用roberta算法性能监控</center>\n",
    "\n",
    "从而我们可以得出一个结论，就是如果我们有充裕的时间的话，可以使用bert模型进行训练数据，倘若我们想比较快的能够显示出结果，那么我们可以使用roberta来进行算法的实现，因为其两者的准确率在epoch达到10次以上后，其实两者的准确率都相当的高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5、个人总结\n",
    "通过本次课程的学习，让我对百度飞桨paddlepaddle平台的使用有了一定的了解，在paddlepaddle上有许多的优秀的开源项目值得我们去参考，去学习，同时也可以下载一些自己感兴趣的开源项目进行download学习，我从自然语言处理这个方向入手深度学习的项目，后面我个人会对图像切割，图像识别，3D重构方向比较感兴趣，若有兴趣的同学，咱们也可以进行一定的交流，这个是[我的主页](https://aistudio.baidu.com/aistudio/usercenter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6、参考\n",
    "1、[BERT模型在文本分类任务上的应用](https://aistudio.baidu.com/aistudio/projectdetail/1991455?channelType=0&channel=0)\n",
    "\n",
    "2、[用BERT做中文邮件内容分类](https://aistudio.baidu.com/aistudio/projectdetail/1988755?channelType=0&channel=0)\n",
    "\n",
    "3、[Grizzly的疫情期间网民情绪识别](https://aistudio.baidu.com/aistudio/projectdetail/3521698)\n",
    "\n",
    "4、[RoBERTa论文详解和代码实战](https://zhuanlan.zhihu.com/p/143064748)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
